{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clv07/stroke-of-luck/blob/Data-import/MI_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuDAMsAImOhg",
        "outputId": "78da5686-7f9f-4668-f795-10cb71f0c421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'accelerated atrial escape rhythm': 233892002, 'abnormal QRS': 164951009, 'atrial escape beat': 251187003, 'accelerated idioventricular rhythm': 61277005, 'accelerated junctional rhythm': 426664006, 'suspect arm ecg leads reversed': 251139008, 'acute myocardial infarction': 57054005, 'acute myocardial ischemia': 413444003, 'anterior ischemia': 426434006, 'anterior myocardial infarction': 54329005, 'atrial bigeminy': 251173003, 'atrial fibrillation and flutter': 195080001, 'atrial hypertrophy': 195126007, 'atrial pacing pattern': 251268003, 'atrial rhythm': 106068003, 'atrial tachycardia': 713422000, 'av block': 233917008, 'atrioventricular dissociation': 50799005, 'atrioventricular junctional rhythm': 29320008, 'atrioventricular  node reentrant tachycardia': 251166008, 'atrioventricular reentrant tachycardia': 233897008, 'blocked premature atrial contraction': 251170000, 'brugada': 418818005, 'brady tachy syndrome': 74615001, 'chronic atrial fibrillation': 426749004, 'countercolockwise rotation': 251199005, 'clockwise or counterclockwise vectorcardiographic loop': 61721007, 'cardiac dysrhythmia': 698247007, 'complete heart block': 27885002, 'congenital incomplete atrioventricular heart block': 204384007, 'coronary heart disease': 53741008, 'chronic myocardial ischemia': 413844008, 'clockwise rotation': 251198002, 'diffuse intraventricular block': 82226007, 'early repolarization': 428417006, 'fusion beats': 13640000, 'fqrs wave': 164942001, 'heart failure': 84114007, 'heart valve disorder': 368009, 'high t-voltage': 251259000, 'indeterminate cardiac axis': 251200008, '2nd degree av block': 195042002, 'mobitz type II atrioventricular block': 426183003, 'inferior ischaemia': 425419005, 'incomplete left bundle branch block': 251120003, 'inferior ST segment depression': 704997005, 'idioventricular rhythm': 49260003, 'junctional escape': 426995002, 'junctional premature complex': 251164006, 'junctional tachycardia': 426648003, 'left atrial abnormality': 253352002, 'left atrial enlargement': 67741000119109, 'left atrial hypertrophy': 446813000, 'lateral ischaemia': 425623009, 'left posterior fascicular block': 445211001, 'left ventricular hypertrophy': 164873001, 'left ventricular high voltage': 55827005, 'left ventricular strain': 370365005, 'myocardial infarction': 164865005, 'myocardial ischemia': 164861001, 'mobitz type i wenckebach atrioventricular block': 54016002, 'nonspecific st t abnormality': 428750005, 'old myocardial infarction': 164867002, 'paroxysmal atrial fibrillation': 282825002, 'prolonged P wave': 251205003, 'paroxysmal supraventricular tachycardia': 67198005, 'paroxysmal ventricular tachycardia': 425856008, 'p wave change': 164912004, 'right atrial abnormality': 253339007, 'r wave abnormal': 164921003, 'right atrial hypertrophy': 446358003, 'right atrial  high voltage': 67751000119106, 'rapid atrial fibrillation': 314208002, 'right ventricular hypertrophy': 89792004, 'sinus atrium to atrial wandering rhythm': 17366009, 'sinoatrial block': 65778007, 'sinus arrest': 5609005, 'sinus node dysfunction': 60423000, 'shortened pr interval': 49578007, 'decreased qt interval': 77867006, 's t changes': 55930002, 'st depression': 429622005, 'st elevation': 164931005, 'st interval abnormal': 164930006, 'supraventricular bigeminy': 251168009, 'supraventricular tachycardia': 426761007, 'transient ischemic attack': 266257000, 'tall p wave': 251223006, 'u wave abnormal': 164937009, 'ventricular bigeminy': 11157007, 'ventricular ectopics': 164884008, 'ventricular escape beat': 75532003, 'ventricular escape rhythm': 81898007, 'ventricular fibrillation': 164896001, 'ventricular flutter': 111288001, 'ventricular hypertrophy': 266249003, 'ventricular pre excitation': 195060002, 'ventricular pacing pattern': 251266004, 'paired ventricular premature complexes': 251182009, 'ventricular tachycardia': 164895002, 'ventricular trigeminy': 251180001, 'wandering atrial pacemaker': 195101003, 'wolff parkinson white pattern': 74390002}\n",
            "{1: 'STATEMENT NOT FOUND', 2: '** * Pediatric ECG analysis * **', 3: '*** Age and gender specific ECG analysis ***', 4: '** Acute Cardiac Syndrome criteria **', 5: 'Report dictated, transcription pending', 6: 'Leads V2, V3, V4, and V6 are interpolated', 8: 'Waveform is valid only when viewed in 4x2.5 format with lead II as the rhythm lead', 9: 'Only the first 5 seconds of lead I are valid', 10: 'Reserved for Database Conversion', 11: 'Reserved for Database Conversion', 12: 'Reserved for Database Conversion', 13: 'The system removed serial comparison statements because', 14: 'this ECG was not analyzed with 12SL', 15: 'the 1st previous ECG was not analyzed with 12SL', 16: 'this patient has a test analyzed with the HEART algorithm', 19: 'Sinus rhythm', 20: '(Atrial rate=', 21: 'Sinus bradycardia', 22: 'Normal sinus rhythm', 23: 'Sinus tachycardia', 24: 'Marked sinus bradycardia', 25: 'Low right atrial bradycardia', 26: 'Low right atrial tachycardia', 27: 'Left atrial bradycardia', 28: 'Left atrial tachycardia', 29: 'Low right atrial rhythm', 30: 'Left atrial rhythm', 31: '(no P-waves found)', 32: 'blocked', 33: 'Accelerated', 34: 'Junctional bradycardia', 41: 'Unusual P axis and short PR, probable junctional bradycardia', 42: 'Unusual P axis and short PR, probable junctional rhythm', 43: 'Unusual P axis and short PR, probable junctional tachycardia', 61: 'Unusual P axis, possible ectopic atrial bradycardia', 62: 'Unusual P axis, possible ectopic atrial rhythm', 63: 'Unusual P axis, possible ectopic atrial tachycardia', 64: 'Ectopic atrial rhythm', 70: 'Bradycardia', 71: 'Tachycardia', 100: 'PR interval', 101: 'with 1st degree AV block', 102: 'with short PR', 103: 'with 2nd degree AV block (Mobitz I)', 104: 'with 2nd degree AV block (Mobitz II)', 105: 'with 2nd degree AV block', 106: 'with complete heart block', 107: 'with variable AV block', 108: 'with AV dissociation', 111: 'with 2nd degree SA block (Mobitz II)', 112: 'with 2nd degree SA block (Mobitz I)', 113: 'with sinus pause', 141: 'with 2:1 AV conduction', 142: 'with 3:1 AV conduction', 143: 'with 4:1 AV conduction', 144: 'with 5:1 AV conduction', 161: 'Atrial fibrillation', 162: 'Atrial flutter', 163: 'Coarse', 164: 'Atrial tachycardia', 171: 'with rapid ventricular response', 172: 'with slow ventricular response', 173: 'with premature ventricular or aberrantly conducted complexes', 174: 'with a competing junctional pacemaker', 175: 'with undetermined rhythm irregularity', 176: 'Irregular', 177: 'with', 178: 'or', 179: 'and', 181: 'with premature ventricular or aberrantly conducted complexes', 183: 'atrial-paced complexes', 184: 'ventricular-paced complexes', 185: 'AV dual-paced complexes', 186: 'atrial-sensed ventricular-paced complexes', 187: 'sinus complexes', 188: 'supraventricular complexes', 189: 'intrinsic complexes', 190: 'with prolonged AV conduction', 211: 'with occasional', 212: 'with frequent', 220: 'supraventricular ectopy', 221: 'premature supraventricular complexes', 222: 'premature atrial complexes', 223: 'premature junctional complexes', 230: 'wide QRS', 231: 'premature ventricular complexes', 232: 'premature ventricular and fusion complexes', 233: 'and consecutive', 234: 'in a pattern of bigeminy', 235: 'Wide QRS tachycardia', 236: 'Narrow QRS tachycardia', 237: 'Wide QRS rhythm', 238: 'Idioventricular rhythm', 241: 'premature ectopic complexes', 242: 'with junctional escape complexes', 243: 'with ventricular escape complexes', 244: 'fusion complexes', 245: 'with retrograde conduction', 246: 'aberrant conduction', 247: 'sinus/atrial capture', 248: 'Ventricular tachycardia', 249: 'Ventricular fibrillation', 251: 'with sinus arrhythmia', 252: 'with marked sinus arrhythmia', 265: 'Probable sinus bradycardia, verify AV conduction', 266: 'Supraventricular tachycardia', 267: 'Junctional rhythm', 268: 'Idioventricular rhythm with AV block', 269: 'Ventricular rhythm', 270: 'Junctional tachycardia', 271: 'Supraventricular tachycardia', 272: 'Ventricular tachycardia (ventricular or supraventricular with aberration)', 273: 'Atrial flutter', 274: 'with ventricular fusion', 275: 'with junctional escape', 276: 'with escape beat', 277: 'with transient ventricular tachycardia', 278: 'with Mobitz I (Wenckebach) block', 279: 'Possible wandering atrial pacemaker', 280: 'Multifocal atrial tachycardia', 281: 'Complete heart block', 282: 'Suspect AV conduction defect', 283: 'with intermittent aberrant ventricular conduction', 284: 'with SA block or transient AV block', 285: 'with sinus arrest or transient AV block', 287: 'Low heart rate, verify AV conduction', 288: 'Atrial flutter with 2 to 1 block', 289: 'Biventricular pacemaker detected', 290: 'Electronic ventricular pacemaker', 291: 'Demand pacemaker, interpretation is based on intrinsic rhythm', 292: 'Electronic atrial pacemaker', 293: 'AV sequential or dual chamber electronic pacemaker', 294: 'Electronic demand pacing', 295: 'Atrial-paced rhythm', 296: 'Ventricular-paced rhythm', 297: 'Atrial-sensed ventricular-paced rhythm', 298: 'AV dual-paced rhythm', 299: 'Undetermined rhythm', 300: 'Ventricular pre-excitation, WPW pattern type A', 302: 'Ventricular pre-excitation, WPW pattern type B', 303: 'with fusion or intermittent ventricular pre-excitation (WPW)', 304: 'Wolff-Parkinson-White', 305: 'Clockwise rotation of the heart, may invalidate criteria for ventricular hypertrophy', 306: 'Counterclockwise rotation of the heart, may invalidate criteria for ventricular hypertrophy', 307: 'Dextrocardia', 320: 'Current undetermined rhythm precludes rhythm comparison, needs review', 321: 'Previous ECG has undetermined rhythm, needs review', 322: 'Vent. rate', 323: 'Rhythm', 324: 'The premature contractions', 325: 'Consecutive', 326: 'with a demand pacemaker', 327: 'Basic rhythm', 350: 'Right atrial enlargement', 360: 'Left atrial enlargement', 369: 'Biatrial enlargement', 370: 'Leftward axis', 371: 'Abnormal left axis deviation', 372: 'Left axis deviation', 380: 'Rightward axis', 381: 'Abnormal right axis deviation', 382: 'Abnormal right superior axis deviation', 383: 'Right axis deviation', 384: 'Right superior axis deviation', 390: 'Indeterminate axis', 391: 'Northwest axis', 395: 'QRS axis', 396: 'shifted left', 397: 'shifted right', 410: 'Low voltage QRS', 411: 'Pulmonary disease pattern', 412: 'S1-S2-S3 pattern, consider pulmonary disease, RVH, or normal variant', 435: 'Brugada pattern, type 1', 436: 'Brugada pattern, type 2', 437: 'Brugada pattern, type 3', 440: 'Right bundle branch block', 441: ', plus right ventricular hypertrophy', 442: 'Right bundle branch block -or- Right ventricular hypertrophy', 445: 'Incomplete right bundle branch block', 446: '.', 450: \"RSR' or QR pattern in V1 suggests right ventricular conduction delay\", 451: \"RSR' pattern in V1\", 460: 'Left bundle branch block', 465: 'Incomplete left bundle branch block', 470: 'Left anterior fascicular block', 471: 'Left posterior fascicular block', 478: '(RBBB and left anterior fascicular block)', 479: '(RBBB and left posterior fascicular block)', 480: '*** Bifascicular block ***', 481: 'Trifascicular block', 482: 'Nonspecific intraventricular block', 487: 'Nonspecific intraventricular conduction delay', 520: 'Right ventricular hypertrophy', 521: 'Right ventricular hypertrophy with repolarization abnormality', 522: ' ', 523: ' ', 524: ' ', 525: 'No Full Text', 530: 'R in aVL', 531: 'Sokolow-Lyon', 532: 'Cornell voltage', 533: 'Cornell product', 534: 'Romhilt-Estes', 540: 'Voltage criteria for left ventricular hypertrophy', 541: 'Left ventricular hypertrophy', 542: 'Minimal voltage criteria for LVH, may be normal variant', 543: 'with QRS widening', 544: 'with repolarization abnormality', 545: 'with QRS widening and repolarization abnormality', 546: '  ', 547: '  ', 548: 'Moderate voltage criteria for LVH, may be normal variant', 549: ' ', 550: ' ', 551: ' ', 552: ' ', 553: '  ', 554: '  ', 555: '  ', 556: '  ', 557: 'No Full Text', 558: 'No Full Text', 559: 'No Full Text', 560: 'No Full Text', 570: 'Biventricular hypertrophy', 571: 'Prominent mid-precordial voltage,', 572: 'Deep Q wave in lead V6,', 573: 'Prominent posterior voltage', 574: 'Prominent lateral voltage', 575: 'Deep Q in lead III', 576: '  ', 577: '  ', 578: '  ', 579: '  ', 580: '  ', 581: '  ', 582: '  ', 583: '  ', 584: '  ', 585: '  ', 586: '  ', 587: '  ', 588: 'No Full Text', 589: 'No Full Text', 590: 'No Full Text', 700: 'Septal infarct', 701: '.', 710: 'Poor R-wave progression', 711: '; consider septal infarct, lead placement, or normal variant', 712: '; consider anteroseptal infarct, lead placement, or normal variant', 713: '; consider anterior infarct, lead placement, or normal variant', 740: 'Anterior infarct', 760: 'Lateral infarct', 780: 'Inferior infarct', 782: '(masked by fascicular block?)', 795: 'with right ventricular involvement', 800: ', with posterior extension', 801: 'Inferior-posterior infarct', 802: 'Posterior infarct', 803: 'Increased R/S ratio in V1, consider early transition or posterior infarct', 805: 'Inferior injury pattern suggests right ventricular involvement, recommend adding leads V3r and V4r to confirm', 806: 'Consider right ventricular involvement in acute inferior infarct', 810: 'Anteroseptal infarct', 820: 'Anterolateral infarct', 821: '** ** ACUTE MI / STEMI ** **', 822: '** ** ACUTE MI / non-STEMI ** **', 823: '** ** Consider ACUTE MI if LBBB is new ** **', 826: '** ** LBBB with primary ST-T abnormality - Consider ACUTE CORONARY SYNDROME (ACS) ** **', 827: '** ** LBBB with primary ST elevation abnormality - PROBABLE ACUTE MI ** **', 828: '** ** Consider ACUTE CORONARY SYNDROME (ACS) ** **', 829: '** ** ACUTE MI ** **', 830: ', possibly acute', 831: ', age undetermined', 832: ', old', 833: ', new', 834: '    ', 835: '    ', 836: '    ', 840: 'Increased evidence of infarction in', 841: 'Questionable change in initial forces of', 842: 'Questionable change in initial forces of', 843: 'Criteria for', 844: '(cited on or before', 845: 'Minimal criteria for', 846: 'Borderline criteria for', 880: '*** QRS contour suggests infarct size is probably', 881: 'very small', 882: 'small', 883: 'moderate', 884: 'large', 885: 'very large', 900: 'Nonspecific ST abnormality', 901: 'Acute pericarditis', 902: 'ST elevation, consider early repolarization, pericarditis, or injury', 903: 'ST elevation, probably due to early repolarization', 904: 'Nonspecific ST elevation', 920: 'Septal injury pattern', 930: 'Anterior injury pattern', 940: 'Lateral injury pattern', 950: 'Inferior injury pattern', 960: 'Anteroseptal injury pattern', 961: 'Anterolateral injury pattern', 962: 'Inferolateral injury pattern', 963: 'ST elevation, consider inferior injury or acute infarct', 964: 'ST elevation, consider anterior injury or acute infarct', 965: 'ST elevation, consider lateral injury or acute infarct', 966: 'ST elevation, consider anterolateral injury or acute infarct', 967: 'ST elevation, consider inferolateral injury or acute infarct', 968: 'ST elevation, consider injury or variant associated with LVH', 1000: 'Early repolarization', 1001: 'Junctional ST depression, probably normal', 1002: 'Junctional ST depression, probably abnormal', 1020: 'ST abnormality, possible digitalis effect', 1021: 'Nonspecific ST abnormality', 1022: 'ST depression, consider subendocardial injury or digitalis effect', 1023: 'Nonspecific ST depression', 1024: 'ST depression, consider subendocardial injury', 1040: 'Marked ST abnormality, possible septal subendocardial injury', 1050: 'Marked ST abnormality, possible anterior subendocardial injury', 1060: 'Marked ST abnormality, possible lateral subendocardial injury', 1070: 'Marked ST abnormality, possible inferior subendocardial injury', 1071: 'Marked ST abnormality, possible inferolateral subendocardial injury', 1080: 'Marked ST abnormality, possible anteroseptal subendocardial injury', 1081: 'Marked ST abnormality, possible anterolateral subendocardial injury', 1082: 'ST depression in', 1083: 'ST elevation in', 1084: 'with strain pattern', 1100: 'ST &', 1104: 'ST no longer depressed in', 1105: 'ST less depressed in', 1106: 'ST more depressed in', 1107: 'ST now depressed in', 1108: 'ST depression has replaced ST elevation in', 1115: 'Questionable change in ST segment', 1116: 'Non-specific change in ST segment in', 1117: 'Non-specific change in ST segment in', 1120: 'ST more elevated in', 1121: 'ST less elevated in', 1122: 'ST elevation now present in', 1123: 'ST no longer elevated in', 1124: 'ST elevation has replaced ST depression in', 1138: 'ST abnormality and', 1139: ', may be secondary to QRS abnormality', 1140: 'Nonspecific T wave abnormality', 1141: 'Nonspecific ST and T wave abnormality', 1142: 'Abnormal QRS-T angle, consider primary T wave abnormality', 1143: 'Prolonged QT', 1144: 'Borderline QT interval', 1145: 'T wave abnormality, consider inferolateral ischemia', 1146: 'QTcB >= 480 msec', 1147: 'QTcFrid >= 480 msec', 1148: 'QTcFram >= 480 msec', 1150: 'T wave abnormality, consider anterior ischemia', 1151: 'Marked T wave abnormality, consider anterior ischemia', 1160: 'T wave abnormality, consider lateral ischemia', 1161: 'Marked T wave abnormality, consider lateral ischemia', 1170: 'T wave abnormality, consider inferior ischemia', 1171: 'Marked T wave abnormality, consider inferior ischemia', 1172: 'Marked T wave abnormality, consider inferolateral ischemia', 1180: 'T wave abnormality, consider anterolateral ischemia', 1181: 'Marked T wave abnormality, consider anterolateral ischemia', 1182: 'T wave inversion in', 1200: 'T waves', 1201: 'T wave amplitude has increased in', 1203: 'T wave amplitude has decreased in', 1207: 'Flat T waves have replaced inverted T waves in', 1208: 'Questionable change in T waves', 1210: 'Flat T waves no longer evident in', 1211: 'Fewer leads exhibit flat T waves in', 1212: 'Flat T waves now evident in', 1213: 'More leads exhibit flat T waves in', 1214: 'Nonspecific T wave abnormality no longer evident in', 1215: 'Nonspecific T wave abnormality now evident in', 1216: 'Nonspecific T wave abnormality, improved in', 1217: 'Nonspecific T wave abnormality, worse in', 1218: 'Nonspecific T wave abnormality has replaced inverted T waves in', 1219: 'Inverted T waves have replaced nonspecific T wave abnormality in', 1220: 'T wave inversion now evident in', 1221: 'T wave inversion more evident in', 1222: 'Inverted T waves have replaced flat T waves in', 1223: 'T wave inversion less evident in', 1224: 'T wave inversion no longer evident in', 1250: 'QT has lengthened', 1251: 'QT has shortened', 1252: 'Although rate has decreased', 1253: 'Although rate has increased', 1254: 'with rate increase', 1255: 'with rate decrease', 1256: 'PREVIOUS ECG IS PRESENT', 1257: 'DATA IS UNCONFIRMED', 1258: 'PREVIOUS ECG IS INCOMPATIBLE', 1259: 'MANUAL COMPARISON REQUIRED', 1260: 'PEDIATRIC ANALYSIS -', 1261: 'by', 1262: 'bpm', 1263: 'When compared with selected ECG of', 1264: 'CURRENT ECG IS INCOMPATIBLE', 1300: 'No previous ECGs available', 1301: 'When compared with ECG of', 1302: 'Poor data quality in current ECG precludes serial comparison', 1303: 'Serial comparison not performed, all previous tracings are of poor data quality', 1304: 'Warning: demographic data different', 1305: 'No significant change was found', 1306: '(Unconfirmed)', 1340: '*** Critical Test Result:', 1342: 'High HR', 1343: 'Low HR', 1346: 'Long QTc', 1360: 'STEMI', 1361: 'ACS / Ischemia', 1362: 'AV Block', 1363: 'Arrhythmia', 1400: 'and', 1401: 'however', 1402: 'however it', 1403: 'Less frequent', 1404: 'More frequent', 1405: 'is no longer', 1406: 'is now', 1407: 'has changed', 1408: 'has not changed', 1409: 'are now', 1410: 'present', 1411: 'have not changed', 1412: 'have changed', 1415: 'has replaced', 1416: 'has increased', 1417: 'has decreased', 1418: 'are no longer', 1419: 'QRS', 1420: 'QRS duration', 1421: 'QRS voltage', 1422: 'Questionable change in', 1423: 'Acute', 1424: 'Serial changes of evolving', 1425: 'Serial changes of', 1426: 'Significant changes have occurred', 1427: 'Manual comparison required, data off line and on volume', 1428: 'Manual comparison required for analog tracing', 1430: 'Manual comparison required, cannot contact main system', 1450: 'Septal leads', 1451: 'Anterior leads', 1452: 'Lateral leads', 1453: 'Inferior leads', 1454: 'Posterior leads', 1455: 'Anteroseptal leads', 1456: 'Anterolateral leads', 1457: 'Inferoposterior leads', 1458: 'Inferolateral leads', 1459: 'Reciprocal', 1460: 'ECG interpretation of ACS is based on presence of symptoms and', 1462: 'ECG not diagnostic for Acute Coronary Syndrome; consider clinical findings', 1500: 'Poor data quality', 1501: 'Powerline interference', 1502: 'Baseline wander', 1503: 'Muscle tremor', 1504: 'Electrode noise', 1505: 'disconnected', 1510: 'in lead', 1511: 'in leads', 1512: 'Possible reversal:', 1513: 'precordial leads', 1537: 'NAP', 1538: 'NST', 1539: 'NAX', 1540: 'RA', 1541: 'LA', 1542: 'RL', 1543: 'LL', 1544: 'Limb lead', 1545: 'H', 1546: 'E', 1547: 'I', 1548: 'M', 1550: 'I', 1551: 'II', 1552: 'V1', 1553: 'V2', 1554: 'V3', 1555: 'V4', 1556: 'V5', 1557: 'V6', 1558: 'V7', 1559: 'V8', 1560: 'V9', 1562: 'V2r', 1563: 'V3r', 1564: 'V4r', 1565: 'V5r', 1566: 'V6r', 1567: 'V7r', 1568: 'V8r', 1569: 'V9r', 1570: 'A1', 1571: 'A2', 1572: 'A3', 1573: 'A4', 1574: 'III', 1575: 'aVR', 1576: 'aVL', 1577: 'aVF', 1578: 'mVR', 1579: 'D', 1580: 'A', 1581: 'J', 1582: 'X', 1583: 'Y', 1584: 'Z', 1585: 'mY', 1586: 'mZ', 1587: 'CC5', 1588: 'CM5', 1592: '; interpretation assumes no reversal', 1595: '*** Suspect electrode reversal:', 1601: 'R', 1602: 'L', 1603: 'N', 1604: 'F', 1605: 'C1', 1606: 'C2', 1607: 'C3', 1608: 'C4', 1609: 'C5', 1610: 'C6', 1611: 'C7', 1612: 'C8', 1613: 'C9', 1615: 'C2r', 1616: 'C3r', 1617: 'C4r', 1618: 'C5r', 1619: 'C6r', 1620: 'C7r', 1621: 'C8r', 1622: 'C9r', 1660: 'No Full Text', 1661: 'No Full Text', 1665: '(', 1666: ')', 1669: '*** Suspect unspecified pacemaker failure', 1670: ', probably digitalis effect', 1671: 'or digitalis effect', 1672: '*** Suspect arm lead reversal, interpretation assumes no reversal', 1673: '*** Poor data quality, interpretation may be adversely affected', 1674: 'Acquisition hardware fault prevents reliable analysis, carefully check ECG record before interpreting', 1675: 'Manual reading required due to inconsistent morphologies', 1676: '** Less than 4 QRS complexes detected, no interpretation possible **', 1677: '*** Memory allocation failure, no ECG interpretation possible ***', 1678: '** No QRS complexes found, no ECG analysis possible **', 1679: '** Nonstandard lead placement, ECG interpretation not available **', 1680: 'Possible', 1682: 'Cannot rule out', 1683: ',', 1684: 'Normal ECG', 1687: 'Otherwise normal ECG', 1693: 'Borderline ECG', 1694: 'Borderline', 1699: 'Abnormal ECG'}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dx_mapping_df = pd.read_csv(\"/mnt/data/dx_mapping_unscored.csv\")\n",
        "dx_to_snomed_dict = dict(zip(dx_mapping_df[\"Dx\"], dx_mapping_df[\"SNOMEDCTCode\"]))\n",
        "\n",
        "statements_df = pd.read_csv(\"/mnt/data/12sl_statements.csv\")\n",
        "statements_df[\"Full Text\"] = statements_df[\"Full Text\"].fillna(\"No Full Text\")\n",
        "statement_number_to_text_dict = dict(zip(statements_df[\"Statement Number\"], statements_df[\"Full Text\"]))\n",
        "\n",
        "\n",
        "print(dx_to_snomed_dict)\n",
        "print(statement_number_to_text_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "-YcrPCe2Qz3g",
        "outputId": "298b458a-0954-4520-c2c4-3e04b0fa9d4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: One or both files missing in ./PTBXL. Skipping.\n",
            "Warning: One or both files missing in ./Shaoxing. Skipping.\n",
            "Warning: One or both files missing in ./CPSC2018. Skipping.\n",
            "Empty DataFrame\n",
            "Columns: [PatientID, Statement_codes, Source]\n",
            "Index: []\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './results.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5bd8945d3baf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mresults_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"./results.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mdf_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# The results.csv sample has headers: patient_num and codes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results.csv'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# STEP 1: READ AND PROCESS EACH 12SL CSV\n",
        "###############################################################################\n",
        "def standardize_patient_id(patient_id):\n",
        "    \"\"\"\n",
        "    Standardizes the PatientID:\n",
        "    - If it ends with '_hr' (e.g., '00001_hr'), it is converted to 'HR00001'.\n",
        "    - If it's already correctly formatted (e.g., 'A00001'), it remains unchanged.\n",
        "    \"\"\"\n",
        "    if isinstance(patient_id, str) and re.match(r\"^\\d+_hr$\", patient_id):\n",
        "        numeric_part = patient_id.split(\"_\")[0]  # Extract '00001'\n",
        "        return f\"HR{numeric_part}\"  # Convert to 'HR00001'\n",
        "    return patient_id  # Leave unchanged if already correct\n",
        "\n",
        "\n",
        "def read_and_process_12sl(folder_path, dataset_name=None):\n",
        "    \"\"\"\n",
        "    Reads and processes the 12SL data from a given folder, which contains:\n",
        "    - meas.csv (has TestID and PatientID)\n",
        "    - 12slv24_stt.csv (has TestID and statement codes)\n",
        "\n",
        "    Parameters:\n",
        "    - folder_path (str): The folder containing the two CSV files.\n",
        "    - dataset_name (str, optional): The dataset name.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Processed DataFrame with 'PatientID', 'Statement_codes', and 'Source'.\n",
        "    \"\"\"\n",
        "    meas_path = os.path.join(folder_path, \"meas.csv\")\n",
        "    stt_path = os.path.join(folder_path, \"12slv24_stt.csv\")\n",
        "\n",
        "    if not os.path.exists(meas_path) or not os.path.exists(stt_path):\n",
        "        print(f\"Warning: One or both files missing in {folder_path}. Skipping.\")\n",
        "        return pd.DataFrame(columns=[\"PatientID\", \"Statement_codes\", \"Source\"])  # Return empty if missing files\n",
        "\n",
        "    # Read meas.csv (contains TestID and PatientID)\n",
        "    df_meas = pd.read_csv(meas_path)\n",
        "\n",
        "    # Read 12slv24_stt.csv (contains TestID and codes)\n",
        "    df_stt = pd.read_csv(stt_path)\n",
        "\n",
        "    # Ensure 'TestID' and 'PatientID' are correctly named\n",
        "    first_col_meas = df_meas.columns[0]  # TestID\n",
        "    second_col_meas = df_meas.columns[1]  # PatientID\n",
        "    df_meas.rename(columns={first_col_meas: \"TestID\", second_col_meas: \"PatientID\"}, inplace=True)\n",
        "\n",
        "    first_col_stt = df_stt.columns[0]  # TestID\n",
        "    df_stt.rename(columns={first_col_stt: \"TestID\"}, inplace=True)\n",
        "\n",
        "    # Merge on TestID, but retain only PatientID\n",
        "    df_raw = pd.merge(df_meas[[\"TestID\", \"PatientID\"]], df_stt, on=\"TestID\", how=\"inner\")\n",
        "\n",
        "    # Standardize PatientID formatting\n",
        "    df_raw[\"PatientID\"] = df_raw[\"PatientID\"].apply(standardize_patient_id)\n",
        "\n",
        "    # Process Statement Codes\n",
        "    if len(df_raw.columns) == 3:  # If only TestID, PatientID, and one column of statements\n",
        "        df_raw[\"Statement_codes\"] = (\n",
        "            df_raw.iloc[:, 2]\n",
        "            .astype(str)\n",
        "            .str.strip(\"|\")\n",
        "            .str.replace(\"|\", \",\")\n",
        "        )\n",
        "    else:  # If multiple statement code columns exist\n",
        "        code_cols = df_raw.columns[2:]\n",
        "        df_raw[\"Statement_codes\"] = df_raw[code_cols].apply(\n",
        "            lambda row: \",\".join(row.dropna().astype(str)), axis=1\n",
        "        )\n",
        "\n",
        "    # Keep only PatientID and Statement_codes\n",
        "    df_processed = df_raw[[\"PatientID\", \"Statement_codes\"]].copy()\n",
        "\n",
        "\n",
        "    # Add Source column\n",
        "    if dataset_name:\n",
        "        df_processed[\"Source\"] = dataset_name\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "# Define dataset folder paths\n",
        "ptbxl_folder = \"./PTBXL\"\n",
        "shaoxing_folder = \"./Shaoxing\"\n",
        "cpsc_folder = \"./CPSC2018\"\n",
        "\n",
        "# Process each dataset\n",
        "df_ptbxl = read_and_process_12sl(ptbxl_folder, dataset_name=\"PTBXL\")\n",
        "df_shaoxing = read_and_process_12sl(shaoxing_folder, dataset_name=\"Shaoxing\")\n",
        "df_cpsc = read_and_process_12sl(cpsc_folder, dataset_name=\"CPSC2018\")\n",
        "\n",
        "# Concatenate all 12SL data.\n",
        "df_12sl_combined = pd.concat([df_ptbxl, df_shaoxing, df_cpsc], ignore_index=True)\n",
        "\n",
        "print(df_12sl_combined.head())\n",
        "\n",
        "###############################################################################\n",
        "# STEP 2: READ AND PREPARE THE results.csv (PHYSICIAN CODES)\n",
        "###############################################################################\n",
        "results_csv = r\"./results.csv\"\n",
        "df_results = pd.read_csv(results_csv)\n",
        "\n",
        "# The results.csv sample has headers: patient_num and codes.\n",
        "# Rename these to \"TestID\" and \"Statements_Phys\" so that we can merge.\n",
        "df_results.rename(columns={\"patient_num\": \"PatientID\", \"codes\": \"Statements_Phys\"}, inplace=True)\n",
        "\n",
        "# It may be necessary to strip any extra spaces from the codes:\n",
        "df_results[\"Statements_Phys\"] = df_results[\"Statements_Phys\"].astype(str).str.strip()\n",
        "\n",
        "###############################################################################\n",
        "# STEP 3: MERGE THE 12SL DATA WITH THE PHYSICIAN DATA\n",
        "###############################################################################\n",
        "# Merge on the patient ID. Use inner join if you want only matching IDs.\n",
        "df_merged = pd.merge(df_12sl_combined, df_results, on=\"PatientID\", how=\"inner\")\n",
        "\n",
        "###############################################################################\n",
        "# STEP 4: DEFINE MI CODE SETS AND CREATE BINARY INDICATORS\n",
        "###############################################################################\n",
        "# Example MI code sets (update these with the actual codes you consider for MI)\n",
        "MI_codes_12SL = {\n",
        "    700, 740, 760, 780, 801, 810, 820, 826, 827, 828, 829,\n",
        "    920, 930, 940, 950, 960, 961, 962, 963, 964, 965, 966, 967, 968,\n",
        "    # ... add others as needed\n",
        "}\n",
        "\n",
        "MI_codes_Phys = {\n",
        "    57054005, 413444003, 426434006, 54329005, 425419005,\n",
        "    425623009, 164865005, 164861001,\n",
        "    # ... add others as needed\n",
        "}\n",
        "# Pairs matching codes from different systems to common condition, formatted\n",
        "# \"Conditon\" : [num_one, num_two ...] , use to identify shared diagnoses\n",
        "# between physicians and 12SL\n",
        "MI_code_mapping = {\n",
        "    \"anterior infarct\": [740, 810, 820, 54329005],\n",
        "    \"septal infarct\": [700, 810],\n",
        "    \"lateral infarct\": [760, 820, 425623009],\n",
        "    \"inferior infarct\": [780, 801, 806, 425419005],\n",
        "    \"posterior infarct\": [801, 802, 803],\n",
        "    \"infarct - ST elevation\": [826, 827, 963, 964, 965, 966, 967, 968],\n",
        "    \"acute MI or injury\": [4, 821, 822, 823, 826, 827, 828, 829, 920, 930, 940, 950, 960, 961, 962, 1361, 57054005, 413444003, 426434006, 164865005, 164861001]\n",
        "}\n",
        "\n",
        "def flag_mi(codes_str, mi_set):\n",
        "    \"\"\"\n",
        "    Given a string of codes separated by commas (possibly with spaces),\n",
        "    returns 1 if any code (converted to int) is in the mi_set, else 0.\n",
        "    \"\"\"\n",
        "    if pd.isna(codes_str) or codes_str.strip() == \"\":\n",
        "        return 0\n",
        "    codes = [c.strip() for c in codes_str.split(\",\") if c.strip()]\n",
        "    for code in codes:\n",
        "        try:\n",
        "            if int(code) in mi_set:\n",
        "                return 1\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return 0\n",
        "\n",
        "def GivenXInspection(df, codes_str, mi_column, mi_code_mapping):\n",
        "    \"\"\"\n",
        "    Given a Merged DF with 1) Patient Labels, 2) 12SL Codes, 3) Physician Codes,\n",
        "    4) MI_12SL, 5) MI_Phys, determine the probability of 12SL correctly identifying\n",
        "    a symptom given a set of codes (i.e., Probability of 12SL detecting MI given ST Elevation).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The merged dataframe containing MI flags.\n",
        "        codes_str (str): A condition category representing the \"given\" signal (e.g., \"anterior infarct\").\n",
        "        mi_column (str): The column name representing the 12SL calculated Condition flag (e.g., \"MI_12SL\").\n",
        "        mi_code_mapping (dict): A dictionary mapping condition categories to a list of associated codes.\n",
        "\n",
        "    Returns:\n",
        "        float: Probability of correct identification.\n",
        "    \"\"\"\n",
        "    # Get the relevant codes for the given condition\n",
        "    condition_codes = set(mi_code_mapping.get(codes_str, []))\n",
        "\n",
        "    # Convert Statements_Phys to sets for efficient lookup\n",
        "    df[\"Statements_Phys_Set\"] = df[\"Statements_Phys\"].apply(lambda x: set(map(int, x.split(','))) if pd.notna(x) else set())\n",
        "\n",
        "    # Identify cases where at least one given condition code is present in the physician's statements\n",
        "    signal_present = df[df[\"Statements_Phys_Set\"].apply(lambda codes: not condition_codes.isdisjoint(codes))]\n",
        "\n",
        "    # Further filter cases where the physician also flagged MI\n",
        "    mi_positive_patients = signal_present.loc[signal_present[\"MI_Phys\"] == 1, \"PatientID\"]\n",
        "\n",
        "    if mi_positive_patients.empty:\n",
        "        return 0  # Avoid division by zero if no cases exist\n",
        "\n",
        "    # Filter 12SL-flagged cases only for identified patients\n",
        "    correct_identifications = df.loc[df[\"PatientID\"].isin(mi_positive_patients), mi_column].sum()\n",
        "\n",
        "    # Calculate probability\n",
        "    probability = correct_identifications / len(mi_positive_patients)\n",
        "\n",
        "    return probability\n",
        "\n",
        "def LabelMapping(df_merged, mi_code_mapping):\n",
        "    \"\"\"\n",
        "    Given a Merged DF with 1) Patient Labels, 2) 12SL Codes, 3) Physician Codes,\n",
        "    4) MI_12SL, 5) MI_Phys, break down the identified signs by 12SL & Physicians\n",
        "    to categorize label percentages in missed and false flag cases.\n",
        "\n",
        "    Args:\n",
        "        df_merged (pd.DataFrame): The merged dataframe containing MI-related flags.\n",
        "        mi_code_mapping (dict): A dictionary mapping condition categories to lists of associated codes.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Two DataFrames -\n",
        "            - df_code_percentages: Breakdown of missed/false flag percentages per code.\n",
        "            - df_signal_percentages: Breakdown of missed/false flag percentages per condition category.\n",
        "    \"\"\"\n",
        "\n",
        "    # Gather cases flagged by physicians but missed by 12SL\n",
        "    df_missed_by_12sl = df_merged[(df_merged[\"MI_Phys\"] == 1) & (df_merged[\"MI_12SL\"] == 0)]\n",
        "\n",
        "    # Gather cases flagged by 12SL but not confirmed by physicians\n",
        "    df_false_id = df_merged[(df_merged[\"MI_12SL\"] == 1) & (df_merged[\"MI_Phys\"] == 0)]\n",
        "\n",
        "    # Function to count occurrences of each code\n",
        "    def count_codes(df):\n",
        "        code_counts = {}\n",
        "        for codes in df[\"Statements_Phys\"].dropna():\n",
        "            for code in map(int, codes.split(',')):  # Convert to int to ensure consistency\n",
        "                code_counts[code] = code_counts.get(code, 0) + 1\n",
        "        return code_counts\n",
        "\n",
        "    # Count occurrences in missed and false flag cases\n",
        "    missed_code_counts = count_codes(df_missed_by_12sl)\n",
        "    false_code_counts = count_codes(df_false_id)\n",
        "\n",
        "    # Convert counts to DataFrames\n",
        "    df_code_percentages = pd.DataFrame([\n",
        "        {\"Code\": code,\n",
        "         \"Missed_Percentage\": missed_code_counts.get(code, 0) / len(df_missed_by_12sl) * 100 if len(df_missed_by_12sl) > 0 else 0,\n",
        "         \"False_Flag_Percentage\": false_code_counts.get(code, 0) / len(df_false_id) * 100 if len(df_false_id) > 0 else 0}\n",
        "        for code in set(missed_code_counts) | set(false_code_counts)  # Include all unique codes\n",
        "    ])\n",
        "\n",
        "    # Aggregate by signal type (condition category)\n",
        "    signal_counts = {}\n",
        "    for condition, codes in mi_code_mapping.items():\n",
        "        signal_counts[condition] = {\n",
        "            \"Missed_Percentage\": sum(missed_code_counts.get(code, 0) for code in codes) / len(df_missed_by_12sl) * 100 if len(df_missed_by_12sl) > 0 else 0,\n",
        "            \"False_Flag_Percentage\": sum(false_code_counts.get(code, 0) for code in codes) / len(df_false_id) * 100 if len(df_false_id) > 0 else 0\n",
        "        }\n",
        "\n",
        "    df_signal_percentages = pd.DataFrame.from_dict(signal_counts, orient=\"index\").reset_index().rename(columns={\"index\": \"Condition\"})\n",
        "\n",
        "    return df_code_percentages, df_signal_percentages\n",
        "# Create MI flags for 12SL and Physician.\n",
        "df_merged[\"MI_12SL\"] = df_merged[\"Statement_codes\"].apply(lambda x: flag_mi(x, MI_codes_12SL))\n",
        "df_merged[\"MI_Phys\"]  = df_merged[\"Statements_Phys\"].apply(lambda x: flag_mi(x, MI_codes_Phys))\n",
        "\n",
        "###############################################################################\n",
        "# STEP 5: INSPECTION / ANALYSIS\n",
        "###############################################################################\n",
        "print(\"Merged Data Sample:\")\n",
        "print(df_merged.head(10))\n",
        "print(df_merged.columns)\n",
        "print(f\"Total merged records: {len(df_merged)}\")\n",
        "print(f\"Total MI flagged by 12SL: {df_merged['MI_12SL'].sum()}\")\n",
        "print(f\"Total MI flagged by Physician: {df_merged['MI_Phys'].sum()}\")\n",
        "\n",
        "# For example, cases flagged by Physician but not by 12SL (False Negative):\n",
        "df_missed_by_12sl = df_merged[(df_merged[\"MI_Phys\"] == 1) & (df_merged[\"MI_12SL\"] == 0)]\n",
        "print(f\"Records where Physician flagged MI but 12SL did not: {len(df_missed_by_12sl)}\")\n",
        "fn = len(df_missed_by_12sl)\n",
        "\n",
        "# Case 2) Cases Flagged by 12SL but not by Physician (False Positive)\n",
        "df_false_id = df_merged[(df_merged[\"MI_12SL\"] == 1) & (df_merged[\"MI_Phys\"] == 0)]\n",
        "fp = len(df_false_id)\n",
        "print(f\"Records where 12SL flagged MI but Physician did not: {len(df_false_id)}\")\n",
        "\n",
        "# Number of true negatives\n",
        "df_negative = df_merged[(df_merged[\"MI_12SL\"] == 0)]\n",
        "print(f\"True negative MI diagnosis: {len(df_negative)}\")\n",
        "tn = len(df_negative)\n",
        "# Number of true positives\n",
        "df_positive = df_merged[(df_merged[\"MI_12SL\"] == 1)]\n",
        "tp = len(df_positive)\n",
        "print(f\"True positive MI diagnosis: {len(df_positive)}\")\n",
        "\n",
        "# False Positive Percent\n",
        "fp_percent = fp / (tn + fp) * 100\n",
        "fn_percent = fn / (tp + fn) * 100\n",
        "print(f\"false positive rate: {fp_percent}\")\n",
        "print(f\"False negative rate: {fn_percent}\")\n",
        "\n",
        "# Case 3)\n",
        "# a) Given STEMI, What is the chance that 12SL calculated it correctly\n",
        "prob_given_STElevation = GivenXInspection(df_merged,\"infarct - ST elevation\",\"MI_12SL\", MI_code_mapping)\n",
        "print(f\"Probability of 12SL identifying MI given ST Elevation: {prob_given_STElevation * 100} %\")\n",
        "\n",
        "# b) Given MI, What is the chance that 12SL calculated it correctly\n",
        "prob_given_lateral = GivenXInspection(df_merged,\"acute MI or injury\",\"MI_12SL\", MI_code_mapping)\n",
        "print(f\"Probability of 12SL identifying MI given MI or Injury: {prob_given_lateral * 100}\")\n",
        "\n",
        "# Conditional Breakdown of Missed regions\n",
        "breakdown = LabelMapping(df_merged, MI_code_mapping)\n",
        "df_by_code = breakdown[0]\n",
        "df_by_signal = breakdown[1]\n",
        "#print(df_by_code.sort_values(by='False_Flag_Percentage', ascending=False))\n",
        "#print(df_by_code.sort_values(by='Missed_Percentage', ascending=False))\n",
        "#print(df_by_signal.sort_values(by='Missed_Percentage', ascending=False))\n",
        "\n",
        "###############################################################################\n",
        "# STEP 6: GRAPHS\n",
        "###############################################################################\n",
        "\n",
        "# GRAPH 1) MISSED PERCENTAGE (DESCENDING) VS CODE\n",
        "ax1 = df_by_code.sort_values(by=\"Missed_Percentage\",ascending=False).head(10).plot.bar(x='Code', y='Missed_Percentage')\n",
        "ax1.set_xlabel(\"Codes\")\n",
        "ax1.set_ylabel(\"False Negative Rate (%)\")\n",
        "plt.title('12SL False Negative Rate (%) versus code')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "# GRAPH 2) FALSE FLAG (DESCENDING) VS CODE\n",
        "ax1 = df_by_code.sort_values(by=\"False_Flag_Percentage\",ascending=False).head(10).plot.bar(x='Code', y='False_Flag_Percentage')\n",
        "ax1.set_xlabel(\"Codes\")\n",
        "ax1.set_ylabel(\"False Positive Rate (%)\")\n",
        "plt.title('12SL False Positive rate (%) versus Code')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# STEP 7: OUTPUT\n",
        "###############################################################################\n",
        "# Update DF to generate two CSV's, Positive 12SL MI Flags and Negative Flags, and 12SL measurements\n",
        "\n",
        "# Trim and Rename\n",
        "df_merged = df_merged.drop('Statements_Phys_Set', axis=1)\n",
        "df_merged.rename(columns={'Statement_codes' : '12SL_Codes', 'Statements_Phys' : 'Phys_Codes'}, inplace= True)\n",
        "\n",
        "# Refine Measurement Files\n",
        "\n",
        "meas_path = \"/content/PTBXL/meas.csv\"\n",
        "df_meas_PT = pd.read_csv(meas_path)\n",
        "df_meas_PT[\"PatientID\"] = df_meas_PT[\"PatientID\"].apply(standardize_patient_id)\n",
        "\n",
        "meas_path = \"/content/Shaoxing/meas.csv\"\n",
        "df_meas_SH = pd.read_csv(meas_path)\n",
        "df_meas_SH[\"PatientID\"] = df_meas_SH[\"PatientID\"].apply(standardize_patient_id)\n",
        "\n",
        "meas_path = \"/content/CPSC2018/meas.csv\"\n",
        "df_meas_CS = pd.read_csv(meas_path)\n",
        "df_meas_CS[\"PatientID\"] = df_meas_CS[\"PatientID\"].apply(standardize_patient_id)\n",
        "\n",
        "df_meas = pd.concat([df_meas_CS,df_meas_PT,df_meas_SH],axis=0)\n",
        "\n",
        "# Join on PatientIDs\n",
        "df_merged = pd.merge(df_merged,df_meas,how=\"left\", on=[\"PatientID\"])\n",
        "\n",
        "# Seperate Into Two CSV's\n",
        "\n",
        "# a) Positive MI\n",
        "df_positive = df_merged[df_merged[\"MI_12SL\"] == 1]\n",
        "# b) Negative MI\n",
        "df_negative = df_merged[df_merged[\"MI_12SL\"] == 0]\n",
        "\n",
        "df_merged.to_csv(\"dataset.csv\", index=False)\n",
        "df_positive.to_csv(\"positive.csv\", index=False)\n",
        "df_negative.to_csv(\"negative.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2QrfqdXNKP2n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dx_mapping_df = pd.read_csv(\"/mnt/data/dx_mapping_unscored.csv\")\n",
        "dx_to_snomed_dict = dict(zip(dx_mapping_df[\"Dx\"], dx_mapping_df[\"SNOMEDCTCode\"]))\n",
        "\n",
        "statements_df = pd.read_csv(\"/mnt/data/12sl_statements.csv\")\n",
        "statements_df[\"Full Text\"] = statements_df[\"Full Text\"].fillna(\"No Full Text\")\n",
        "statement_number_to_text_dict = dict(zip(statements_df[\"Statement Number\"], statements_df[\"Full Text\"]))\n",
        "\n",
        "def translate_codes_to_text(df, code_column, mapping_dict):\n",
        "    df[code_column] = df[code_column].apply(lambda x: mapping_dict.get(x, x) if pd.notna(x) else x)\n",
        "    return df\n",
        "\n",
        "def plot_graph(df, x_column, y_column, title, xlabel, ylabel):\n",
        "    df_translated = translate_codes_to_text(df, x_column, dx_to_snomed_dict)\n",
        "    ax = df_translated.sort_values(by=y_column, ascending=False).head(10).plot.bar(x=x_column, y=y_column)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhlDqXtE5aDz"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# STEP 8: TEST\n",
        "###############################################################################\n",
        "# Test cases to verify datasets are being properly joined\n",
        "\n",
        "def read_file(folder, file):\n",
        "    return pd.read_csv(os.path.join(folder, file))\n",
        "\n",
        "meas_name = \"meas.csv\"\n",
        "stt_name = \"12slv24_stt.csv\"\n",
        "sources = [\"Shaoxing\", \"PTBXL\", \"CPSC2018\"]\n",
        "\n",
        "datasets = {source: (read_file(source, meas_name), read_file(source, stt_name)) for source in sources}\n",
        "\n",
        "output_df = pd.read_csv(\"merged_output.csv\")\n",
        "physician_diag = pd.read_csv(\"results.csv\")\n",
        "\n",
        "original_id = pd.concat([df[\"PatientID\"] for df, _ in datasets.values()])\n",
        "output_id = output_df[\"PatientID\"]\n",
        "\n",
        "# No missing entries\n",
        "assert sorted(original_id) == sorted(output_id), \"Missing entries\"\n",
        "\n",
        "# Duplicate Patient ID\n",
        "# assert len(output_id) == len(output_id.unique()), \"Duplicate Patient ID\" # failing\n",
        "visit, dup = set(), []\n",
        "for id in output_id:\n",
        "    dup.append(id) if id in visit else visit.add(id)\n",
        "\n",
        "print(f\"Duplicate Patient ID: {dup}\")\n",
        "for id in dup:\n",
        "    print(output_df[output_df[\"PatientID\"] == id])\n",
        "\n",
        "# Correct format of Patient ID\n",
        "id_format = r\"^[A-Z]+\\d+$\"\n",
        "assert output_df[\"PatientID\"].str.match(id_format).all(), \"Incorrect format Patient ID\"\n",
        "\n",
        "def convert_format(s):\n",
        "    return re.sub(r\"^(\\d+)_([a-z]+)$\", lambda m: m.group(2).upper() + m.group(1), s)\n",
        "\n",
        "for source, (meas, stt) in datasets.items():\n",
        "    output = output_df[output_df[\"Source\"] == source]\n",
        "    original_id = meas[\"PatientID\"].apply(convert_format) if source == \"PTBXL\" else meas[\"PatientID\"]\n",
        "    assert sorted(output_id.tolist()) == sorted(original_id.tolist()), f\"Mismatch found for {source}\"\n",
        "\n",
        "    # Physician code mapping\n",
        "    for id in output_id:\n",
        "        original_phys = physician_diag[physician_diag[\"patient_num\"] == id][\"codes\"]\n",
        "        output_phys = output_df[output_df[\"PatientID\"] == id][\"Statements_Phys\"]\n",
        "        assert not output_phys.empty, f\"Physicians statement code missing for Patient ID {id}\"\n",
        "        assert sorted(original_phys) == sorted(output_phys), f\"Incorrect physician statement code mapping for Patient ID {id}\"\n",
        "\n",
        "    # 12SL code mapping\n",
        "    merged_df = meas.merge(stt, on=\"TestID\", how=\"inner\")[['PatientID', 'Statements']]\n",
        "    for id in output_id:\n",
        "        original_12sl = merged_df[merged_df[\"PatientID\"] == id][\"Statements\"]\n",
        "        output_12sl = output_df[output_df[\"PatientID\"] == id][\"Statements_12SL\"]\n",
        "        assert not output_12sl.empty, f\"12SL statement code missing for Patient ID {id}\"\n",
        "        assert sorted(original_12sl) == sorted(output_12sl), f\"Incorrect 12SL statement code mapping for Patient ID {id}\"\n",
        "\n",
        "    # Physician diagnosis\n",
        "    for id in output_id:\n",
        "        output_phys = output_df[output_df[\"PatientID\"] == id][\"Statements_Phys\"]\n",
        "        output_phys = [code for code in output_phys if code in MI_codes_Phys]\n",
        "        output_phys_mi = 1 if output_phys else 0\n",
        "        assert \"MI_Phys\" in output_df.columns, f\"Missing MI_Phys column for Patient ID {id}\"\n",
        "        assert output_df.loc[output_df[\"PatientID\"] == id, \"MI_Phys\"].values[0] == output_phys_mi, f\"Incorrect Physician MI flag for Patient ID {id}\"\n",
        "\n",
        "    # 12SL diagnosis\n",
        "    for id in output_id:\n",
        "        output_12sl = output_df[output_df[\"PatientID\"] == id][\"Statement_codes\"]\n",
        "        output_12sl = [code for code in output_12sl if code in MI_codes_12SL]\n",
        "        output_12sl_mi = 1 if output_12sl else 0\n",
        "        assert \"MI_12SL\" in output_df.columns, f\"Missing MI_12SL column for Patient ID {id}\"\n",
        "        assert output_df.loc[output_df[\"PatientID\"] == id, \"MI_12SL\"].values[0] == output_12sl_mi, f\"Incorrect 12SL MI flag for Patient ID {id}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMTIoYpluuAW",
        "outputId": "b61cb31b-48af-465c-c37a-1cce7e141ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate Patient ID: ['A0001', 'Q0001']\n",
            "     PatientID           Statement_codes    Source Statements_Phys  MI_12SL  \\\n",
            "2751     A0001  161,171,440,700,831,1699  Shaoxing        59118001        1   \n",
            "2753     A0001                   21,1687  Shaoxing        59118001        0   \n",
            "\n",
            "      MI_Phys Statements_Phys_Set  \n",
            "2751        0          {59118001}  \n",
            "2753        0          {59118001}  \n",
            "     PatientID Statement_codes    Source       Statements_Phys  MI_12SL  \\\n",
            "2752     Q0001    21,1140,1699  Shaoxing  164867002, 427084000        0   \n",
            "9630     Q0001         21,1687  Shaoxing  164867002, 427084000        0   \n",
            "\n",
            "      MI_Phys     Statements_Phys_Set  \n",
            "2752        0  {427084000, 164867002}  \n",
            "9630        0  {427084000, 164867002}  \n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
