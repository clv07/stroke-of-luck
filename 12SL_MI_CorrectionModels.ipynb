{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('positive.csv', na_values=['NULL'])\n",
    "\n",
    "df2 = pd.read_csv('negative.csv', na_values=['NULL'])\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df['AcquisitionDateTime_DT'] = pd.to_datetime(df['AcquisitionDateTime_DT'])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.info())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "y = df[\"MI_Phys\"]\n",
    "X = df.drop(columns=[\"PatientID\", \"12SL_Codes\", \"Phys_Codes\", \"TestID\", \"Source\", \n",
    "                     \"Gender\", \"PatientAge\", \"AcquisitionDateTime_DT\", \"MI_Phys\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Save the original algorithm's prediction for test set before dropping it\n",
    "y_12SL = X_test[\"MI_12SL\"]\n",
    "\n",
    "# Split data based on MI_12SL classification\n",
    "X_train_pos = X_train[X_train[\"MI_12SL\"] == 1].drop(columns=[\"MI_12SL\"])\n",
    "X_train_neg = X_train[X_train[\"MI_12SL\"] == 0].drop(columns=[\"MI_12SL\"])\n",
    "X_test_pos = X_test[X_test[\"MI_12SL\"] == 1].drop(columns=[\"MI_12SL\"])\n",
    "X_test_neg = X_test[X_test[\"MI_12SL\"] == 0].drop(columns=[\"MI_12SL\"])\n",
    "\n",
    "# Ensure y labels match the correct samples\n",
    "y_train_pos = y_train.loc[X_train_pos.index]  # True positives or false positives\n",
    "y_train_neg = y_train.loc[X_train_neg.index]  # True negatives or false negatives\n",
    "y_test_pos = y_test.loc[X_test_pos.index]\n",
    "y_test_neg = y_test.loc[X_test_neg.index]\n",
    "\n",
    "X_train = X_train.drop(columns=[\"MI_12SL\"])\n",
    "X_test = X_test.drop(columns=[\"MI_12SL\"])\n",
    "\n",
    "# Extract MI_12SL predictions\n",
    "y_12SL_pos = y_12SL.loc[X_test_pos.index]  # Original classifier's labels\n",
    "y_12SL_neg = y_12SL.loc[X_test_neg.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of positive and negative values based on MI_Phys\n",
    "print(df1['MI_Phys'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'dart',\n",
    "    'colsample_bytree': 0.9889,\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.0807,\n",
    "    'random_state': 42,\n",
    "    'verbose': -1,\n",
    "    'num_leaves': 106,\n",
    "    'min_child_weight': 1,\n",
    "    'max_depth' : 7,\n",
    "    'reg_alpha' : 0.7909,\n",
    "    'reg_lambda' : 0.61348,\n",
    "    'subsample' : 0.903,\n",
    "    # 'metric': 'auc', \n",
    "    'scale_pos_weight': 3.537\n",
    "    }\n",
    "model_pos = (lgb.LGBMClassifier(**lgb_params))\n",
    "model_pos.fit(X_train_pos, y_train_pos)\n",
    "y_pred_pos = model_pos.predict(X_test_pos)\n",
    "score = f1_score(y_test_pos, y_pred_pos, average='micro')\n",
    "print(\"Final F1 score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "X = X_train_pos\n",
    "y = y_train_pos\n",
    "\n",
    "# Hyperparameter search space\n",
    "space = {\n",
    "    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'rf']),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 15, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "    'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 1, 10),\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        objective='binary',\n",
    "        # metric='auc',\n",
    "        verbose=-1,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(clf, X, y, scoring='f1', cv=cv).mean()\n",
    "    \n",
    "    return {'loss': 1 - score, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,  \n",
    "    trials=trials,\n",
    "    rstate=np.random.Generator(np.random.PCG64(42))\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model with the parameters from the dictionary with the model parameters from \"best\"\n",
    "boosting_type = ['gbdt', 'dart', 'rf']\n",
    "best['boosting_type'] = boosting_type[best['boosting_type']]\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': best['boosting_type'],\n",
    "    'colsample_bytree': best['colsample_bytree'],\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': best['learning_rate'],\n",
    "    'random_state': 42,\n",
    "    'verbose': -1,\n",
    "    'num_leaves': int(best['num_leaves']),\n",
    "    'min_child_weight': int(best['min_child_weight']),\n",
    "    'max_depth' : int(best['max_depth']),\n",
    "    'reg_alpha' : best['reg_alpha'],\n",
    "    'reg_lambda' : best['reg_lambda'],\n",
    "    'subsample' : best['subsample'],\n",
    "    # 'metric': 'auc', \n",
    "    'scale_pos_weight': best['scale_pos_weight'],\n",
    "    }\n",
    "\n",
    "model_pos = (lgb.LGBMClassifier(**lgb_params))\n",
    "model_pos.fit(X_train_pos, y_train_pos)\n",
    "y_pred_pos = model_pos.predict(X_test_pos)\n",
    "score = f1_score(y_test_pos, y_pred_pos, average='micro')\n",
    "print(\"Final F1 score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna optimization\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "X = X_train_pos\n",
    "y = y_train_pos\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the search space using trial.suggest_\n",
    "    boosting_type = trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'rf'])\n",
    "    num_leaves = trial.suggest_int('num_leaves', 20, 150)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 15)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.2, log=True)\n",
    "    subsample = trial.suggest_float('subsample', 0.6, 1.0)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "    reg_alpha = trial.suggest_float('reg_alpha', 0, 1.0)\n",
    "    reg_lambda = trial.suggest_float('reg_lambda', 0, 1.0)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "    scale_pos_weight = trial.suggest_float('scale_pos_weight', 1.0, 10.0)\n",
    "\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        objective='binary',\n",
    "        boosting_type=boosting_type,\n",
    "        num_leaves=num_leaves,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        min_child_weight=min_child_weight,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = cross_val_score(clf, X, y, scoring='f1', cv=cv)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "    return 1 - mean_f1  # Optuna minimizes the objective\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1) \n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial)\n",
    "print(\"Best F1 Score:\", 1 - study.best_value)\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model with optuna best params\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': study.best_params['boosting_type'],\n",
    "    'colsample_bytree': study.best_params['colsample_bytree'],\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': study.best_params['learning_rate'],\n",
    "    'random_state': 42,\n",
    "    'verbose': -1,\n",
    "    'num_leaves': int(study.best_params['num_leaves']),\n",
    "    'min_child_weight': int(study.best_params['min_child_weight']),\n",
    "    'max_depth' : int(study.best_params['max_depth']),\n",
    "    'reg_alpha' : study.best_params['reg_alpha'],\n",
    "    'reg_lambda' : study.best_params['reg_lambda'],\n",
    "    'subsample' : study.best_params['subsample'],\n",
    "    # 'metric': 'auc', \n",
    "    'scale_pos_weight': study.best_params['scale_pos_weight'],\n",
    "}\n",
    "\n",
    "model_pos = (lgb.LGBMClassifier(**lgb_params))\n",
    "model_pos.fit(X_train_pos, y_train_pos)\n",
    "y_pred_pos = model_pos.predict(X_test_pos)\n",
    "score = f1_score(y_test_pos, y_pred_pos, average='micro')\n",
    "print(\"Final F1 score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities or classes from your second model trained on 12SL positives\n",
    "y_pos_pred_proba = model_pos.predict_proba(X_test_pos)[:, 1]\n",
    "threshold = 0.5\n",
    "y_pos_pred = (y_pos_pred_proba >= threshold).astype(int)\n",
    "\n",
    "# Keep alignment with X_test_pos\n",
    "results_pos = pd.DataFrame({\n",
    "    'true': y_test_pos,\n",
    "    'pred': y_pos_pred,\n",
    "    'prob': y_pos_pred_proba\n",
    "}, index=X_test_pos.index)\n",
    "\n",
    "\n",
    "false_positives_pos = results_pos[(results_pos['pred'] == 1) & (results_pos['true'] == 0)]\n",
    "false_negatives_pos = results_pos[(results_pos['pred'] == 0) & (results_pos['true'] == 1)]\n",
    "\n",
    "X_fp_pos = X_test_pos.loc[false_positives_pos.index]\n",
    "X_fn_pos = X_test_pos.loc[false_negatives_pos.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "background = X_train_pos.sample(100, random_state=42)\n",
    "\n",
    "# Create the explainer with reference data\n",
    "explainer = shap.TreeExplainer(\n",
    "    model_pos,\n",
    "    data=background,\n",
    "    model_output='probability',\n",
    "    feature_perturbation='interventional'\n",
    ")\n",
    "\n",
    "# Now generate SHAP values without error\n",
    "shap_values = explainer.shap_values(X_test_pos)\n",
    "\n",
    "# SHAP values for FPs and FNs in the positive model\n",
    "shap_values_fp = explainer.shap_values(X_fp_pos)\n",
    "shap_values_fn = explainer.shap_values(X_fn_pos)\n",
    "\n",
    "shap.initjs()\n",
    "shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values_fp[0], feature_names=X_fp_pos.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_fp, X_fp_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = X_train_pos.sample(100, random_state=42)\n",
    "\n",
    "# Create the explainer with reference data\n",
    "explainer = shap.TreeExplainer(\n",
    "    model_pos,\n",
    "    data=background,\n",
    "    model_output='probability',\n",
    "    feature_perturbation='interventional'\n",
    ")\n",
    "\n",
    "# Now generate SHAP values without error\n",
    "shap_values = explainer.shap_values(X_test_pos)\n",
    "\n",
    "# SHAP values for FPs and FNs in the positive model\n",
    "shap_values_fp = explainer.shap_values(X_fp_pos)\n",
    "shap_values_fn = explainer.shap_values(X_fn_pos)\n",
    "\n",
    "shap.initjs()\n",
    "shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values_fn[0], feature_names=X_fn_pos.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_fn, X_fn_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature = 'PFull_Area_I'\n",
    "plt.hist(X_test_pos[feature], bins=50, alpha=0.5, label='All 12SL Positives')\n",
    "plt.axvline(X_fp_pos[feature].mean(), color='red', label='FP Mean')\n",
    "plt.axvline(X_fn_pos[feature].mean(), color='green', label='FN Mean')\n",
    "plt.legend()\n",
    "plt.title(f'Distribution of {feature}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature = 'VentricularRate'\n",
    "plt.hist(X_test_pos[feature], bins=50, alpha=0.5, label='All 12SL Positives')\n",
    "plt.axvline(X_fp_pos[feature].mean(), color='red', label='FP Mean')\n",
    "plt.axvline(X_fn_pos[feature].mean(), color='green', label='FN Mean')\n",
    "plt.legend()\n",
    "plt.title(f'Distribution of {feature}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#confusion matrix 12SL (no model)\n",
    "\n",
    "cm = confusion_matrix(y_test_pos, y_12SL_pos)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"No MI\", \"MI\"], yticklabels=[\"No MI\", \"MI\"])\n",
    "plt.title(\"Confusion Matrix: Original 12SL Classifier (Positives)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#confusion matrix (from model)\n",
    "\n",
    "cm = confusion_matrix(y_test_pos, y_pred_pos)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"No MI\", \"MI\"], yticklabels=[\"No MI\", \"MI\"])\n",
    "plt.title(\"Confusion Matrix: Our Model (Positives)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "y_true = y_test_pos  # Ground truth for positive subset\n",
    "baseline_pred = y_12SL.loc[y_test_pos.index]  # 12SL classifier predictions for the same subset\n",
    "model_pred = y_pred_pos  # Predictions from your trained model\n",
    "\n",
    "sensitivity_baseline = recall_score(y_true, baseline_pred)\n",
    "precision_baseline = precision_score(y_true, baseline_pred)\n",
    "\n",
    "sensitivity_model = recall_score(y_true, model_pred)\n",
    "precision_model = precision_score(y_true, model_pred)\n",
    "\n",
    "print(\"Baseline Sensitivity (Recall):\", sensitivity_baseline)\n",
    "print(\"Baseline Precision (PPV):\", precision_baseline)\n",
    "print(\"Model Sensitivity (Recall):\", sensitivity_model)\n",
    "print(\"Model Precision (PPV):\", precision_model)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(sensitivity_baseline, precision_baseline, \n",
    "            label='12SL Baseline', color='blue', marker='o', s=100)\n",
    "plt.scatter(sensitivity_model, precision_model, \n",
    "            label='Model Predictions', color='red', marker='x', s=100)\n",
    "\n",
    "plt.xlabel('Sensitivity (Recall)')\n",
    "plt.ylabel('Positive Predictive Value (Precision)')\n",
    "plt.title('Sensitivity vs. Positive Predictive Value')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "importances = model_pos.feature_importances_\n",
    "feature_names = X_train.columns if hasattr(X_train, 'columns') else np.arange(len(importances))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(importances)), importances)\n",
    "plt.xticks(range(len(importances)), feature_names, rotation='vertical')\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = model_pos.booster_\n",
    "importances = booster.feature_importance(importance_type='split')\n",
    "feature_names = booster.feature_name()\n",
    "\n",
    "lgb.plot_importance(booster, importance_type='gain', max_num_features=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_cols = df.drop(columns=['PatientID', '12SL_Codes', 'Phys_Codes','TestID', 'Source', 'MI_12SL', 'Gender', 'PatientAge', 'AcquisitionDateTime_DT', 'MI_Phys'])\n",
    "# drop NaN values\n",
    "df_cols = df_cols.dropna()\n",
    "X_scaled = scaler.fit_transform(df_cols)\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)  # keep 95% of variance\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Number of components selected:\", pca.n_components_)\n",
    "print(\"Total variance explained:\", pca.explained_variance_ratio_.sum())\n",
    "print(\"Principal components shape:\", principal_components.shape)\n",
    "#column names from original data\n",
    "feature_names = df_cols.columns\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=[f\"PC{i}\" for i in range(1, pca.n_components_ + 1)])\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_)\n",
    "plt.title(\"Explained Variance Ratio of Principal Components\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.xticks(range(1, pca.n_components_ + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.1,\n",
    "    'random_state': 42,\n",
    "    'verbose': -1,\n",
    "    'num_leaves': 127,\n",
    "    'scale_pos_weight': 10,\n",
    "    }\n",
    "model_neg = (lgb.LGBMClassifier(**lgb_params))\n",
    "model_neg.fit(X_train_neg, y_train_neg)\n",
    "y_pred_neg = model_neg.predict(X_test_neg)\n",
    "score = f1_score(y_test_neg, y_pred_neg, average='micro')\n",
    "print(\"Final F1 score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "X = X_train_neg\n",
    "y = y_train_neg\n",
    "\n",
    "# Hyperparameter search space\n",
    "space = {\n",
    "    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart']),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 15, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "    'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 1, 10),\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        objective='binary',\n",
    "        # metric='auc',\n",
    "        verbose=-1,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(clf, X, y, scoring='f1', cv=cv).mean()\n",
    "    \n",
    "    return {'loss': 1 - score, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,  \n",
    "    trials=trials,\n",
    "    rstate=np.random.Generator(np.random.PCG64(42))\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model with the parameters from the dictionary with the model parameters from \"best\"\n",
    "boosting_type = ['gbdt', 'dart']\n",
    "best['boosting_type'] = boosting_type[best['boosting_type']]\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': best['boosting_type'],\n",
    "    'colsample_bytree': best['colsample_bytree'],\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': best['learning_rate'],\n",
    "    'random_state': 42,\n",
    "    'verbose': -1,\n",
    "    'num_leaves': int(best['num_leaves']),\n",
    "    'min_child_weight': int(best['min_child_weight']),\n",
    "    'max_depth' : int(best['max_depth']),\n",
    "    'reg_alpha' : best['reg_alpha'],\n",
    "    'reg_lambda' : best['reg_lambda'],\n",
    "    'subsample' : best['subsample'],\n",
    "    # 'metric': 'auc', \n",
    "    'scale_pos_weight': best['scale_pos_weight'],\n",
    "    }\n",
    "\n",
    "model_neg = (lgb.LGBMClassifier(**lgb_params))\n",
    "model_neg.fit(X_train_neg, y_train_neg)\n",
    "y_pred_neg = model_neg.predict(X_test_neg)\n",
    "score = f1_score(y_test_neg, y_pred_neg, average='micro')\n",
    "print(\"Final F1 score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "X = X_train_neg\n",
    "y = y_train_neg\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the search space\n",
    "    boosting_type = trial.suggest_categorical('boosting_type', ['gbdt', 'dart'])\n",
    "    num_leaves = trial.suggest_int('num_leaves', 20, 150)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 15)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.2, log=True)\n",
    "    subsample = trial.suggest_float('subsample', 0.6, 1.0)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "    reg_alpha = trial.suggest_float('reg_alpha', 0, 1.0)\n",
    "    reg_lambda = trial.suggest_float('reg_lambda', 0, 1.0)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "    scale_pos_weight = trial.suggest_float('scale_pos_weight', 1.0, 10.0)\n",
    "\n",
    "    # Construct classifier\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        objective='binary',\n",
    "        boosting_type=boosting_type,\n",
    "        num_leaves=num_leaves,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        min_child_weight=min_child_weight,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = cross_val_score(clf, X, y, scoring='f1', cv=cv)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "    return 1 - mean_f1  # Optuna minimizes the objective\n",
    "\n",
    "# Create and run the Optuna study\n",
    "study = optuna.create_study(direction='minimize') \n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial)\n",
    "print(\"Best F1 Score:\", 1 - study.best_value)\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model with optuna best params\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': study.best_params['boosting_type'],\n",
    "    'colsample_bytree': study.best_params['colsample_bytree'],\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': study.best_params['learning_rate'],\n",
    "    'random_state': 42,\n",
    "    'verbose': -1,\n",
    "    'num_leaves': int(study.best_params['num_leaves']),\n",
    "    'min_child_weight': int(study.best_params['min_child_weight']),\n",
    "    'max_depth' : int(study.best_params['max_depth']),\n",
    "    'reg_alpha' : study.best_params['reg_alpha'],\n",
    "    'reg_lambda' : study.best_params['reg_lambda'],\n",
    "    'subsample' : study.best_params['subsample'],\n",
    "    # 'metric': 'auc', \n",
    "    'scale_pos_weight': study.best_params['scale_pos_weight'],\n",
    "}\n",
    "\n",
    "model_neg = (lgb.LGBMClassifier(**lgb_params))\n",
    "model_neg.fit(X_train_neg, y_train_neg)\n",
    "y_pred_neg = model_neg.predict(X_test_neg)\n",
    "score = f1_score(y_test_neg, y_pred_neg, average='micro')\n",
    "print(\"Final F1 score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict probabilities or classes from your second model trained on 12SL positives\n",
    "y_pos_pred_proba = model_neg.predict_proba(X_test_neg)[:, 1]\n",
    "threshold = 0.5\n",
    "y_pos_pred = (y_pos_pred_proba >= threshold).astype(int)\n",
    "\n",
    "# Keep alignment with X_test_pos\n",
    "results_neg = pd.DataFrame({\n",
    "    'true': y_test_neg,\n",
    "    'pred': y_pos_pred,\n",
    "    'prob': y_pos_pred_proba\n",
    "}, index=X_test_neg.index)\n",
    "\n",
    "\n",
    "false_positives_neg = results_neg[(results_neg['pred'] == 1) & (results_neg['true'] == 0)]\n",
    "false_negatives_neg = results_neg[(results_neg['pred'] == 0) & (results_neg['true'] == 1)]\n",
    "\n",
    "X_fp_neg = X_test_neg.loc[false_positives_neg.index]\n",
    "X_fn_neg = X_test_neg.loc[false_negatives_neg.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "background = X_train_neg.sample(100, random_state=42)\n",
    "\n",
    "# Create the explainer with reference data\n",
    "explainer = shap.TreeExplainer(\n",
    "    model_neg,\n",
    "    data=background,\n",
    "    model_output='probability',\n",
    "    feature_perturbation='interventional'\n",
    ")\n",
    "\n",
    "# Now generate SHAP values without error\n",
    "shap_values = explainer.shap_values(X_test_neg)\n",
    "\n",
    "# SHAP values for FPs and FNs in the positive model\n",
    "shap_values_fp = explainer.shap_values(X_fp_neg)\n",
    "shap_values_fn = explainer.shap_values(X_fn_neg)\n",
    "\n",
    "shap.initjs()\n",
    "shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values_fp[0], feature_names=X_fp_neg.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_fp, X_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "background = X_train_neg.sample(100, random_state=42)\n",
    "\n",
    "# Create the explainer with reference data\n",
    "explainer = shap.TreeExplainer(\n",
    "    model_pos,\n",
    "    data=background,\n",
    "    model_output='probability',\n",
    "    feature_perturbation='interventional'\n",
    ")\n",
    "\n",
    "# Now generate SHAP values without error\n",
    "shap_values = explainer.shap_values(X_test_neg)\n",
    "\n",
    "# SHAP values for FPs and FNs in the positive model\n",
    "shap_values_fp = explainer.shap_values(X_fp_neg)\n",
    "shap_values_fn = explainer.shap_values(X_fn_neg)\n",
    "\n",
    "shap.initjs()\n",
    "shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values_fn[0], feature_names=X_fn_neg.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_fn, X_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# feature = 'Some_Feature_Name'\n",
    "# plt.hist(X_test_neg[feature], bins=50, alpha=0.5, label='All 12SL Positives')\n",
    "# plt.axvline(X_fp_neg[feature].mean(), color='red', label='FP Mean')\n",
    "# plt.axvline(X_fn_neg[feature].mean(), color='green', label='FN Mean')\n",
    "# plt.legend()\n",
    "# plt.title(f'Distribution of {feature}')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix 12SL (no model)\n",
    "\n",
    "cm = confusion_matrix(y_test_neg, y_12SL_neg)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"No MI\", \"MI\"], yticklabels=[\"No MI\", \"MI\"])\n",
    "plt.title(\"Confusion Matrix: Original 12SL Classifier (Negatives)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "#confusion matrix (from model)\n",
    "\n",
    "cm = confusion_matrix(y_test_neg, y_pred_neg)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"No MI\", \"MI\"], yticklabels=[\"No MI\", \"MI\"])\n",
    "plt.title(\"Confusion Matrix: Our Model (Negatives)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "y_true = y_test_neg  # Ground truth for positive subset\n",
    "baseline_pred = y_12SL.loc[y_test_neg.index]  # 12SL classifier predictions for the same subset\n",
    "model_pred = y_pred_neg  # Predictions from your trained model\n",
    "\n",
    "sensitivity_baseline = recall_score(y_true, baseline_pred)\n",
    "precision_baseline = precision_score(y_true, baseline_pred)\n",
    "\n",
    "sensitivity_model = recall_score(y_true, model_pred)\n",
    "precision_model = precision_score(y_true, model_pred)\n",
    "\n",
    "print(\"Baseline Sensitivity (Recall):\", sensitivity_baseline)\n",
    "print(\"Baseline Precision (PPV):\", precision_baseline)\n",
    "print(\"Model Sensitivity (Recall):\", sensitivity_model)\n",
    "print(\"Model Precision (PPV):\", precision_model)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(sensitivity_baseline, precision_baseline, \n",
    "            label='12SL Baseline', color='blue', marker='o', s=100)\n",
    "plt.scatter(sensitivity_model, precision_model, \n",
    "            label='Model Predictions', color='red', marker='x', s=100)\n",
    "\n",
    "plt.xlabel('Sensitivity (Recall)')\n",
    "plt.ylabel('Positive Predictive Value (Precision)')\n",
    "plt.title('Sensitivity vs. Positive Predictive Value')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model_neg.feature_importances_\n",
    "feature_names = X_train.columns if hasattr(X_train, 'columns') else np.arange(len(importances))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(importances)), importances)\n",
    "plt.xticks(range(len(importances)), feature_names, rotation='vertical')\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = model_neg.booster_\n",
    "importances = booster.feature_importance(importance_type='split')\n",
    "feature_names = booster.feature_name()\n",
    "\n",
    "lgb.plot_importance(booster, importance_type='gain', max_num_features=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "#Compute performance for the original algorithm (MI_12SL)\n",
    "y_true = y_test\n",
    "baseline_pred = y_12SL \n",
    "\n",
    "sensitivity_baseline = recall_score(y_true, baseline_pred)\n",
    "precision_baseline = precision_score(y_true, baseline_pred)\n",
    "\n",
    "#Generate Combined Model Predictions\n",
    "y_pred_pos = model_pos.predict(X_test_pos)\n",
    "y_pred_neg = model_neg.predict(X_test_neg)\n",
    "\n",
    "# Merge the corrected predictions into a single output\n",
    "y_pred_combined = y_12SL.copy()  # original MI_12SL predictions\n",
    "y_pred_combined.loc[X_test_pos.index] = y_pred_pos  # Replace with TP model results\n",
    "y_pred_combined.loc[X_test_neg.index] = y_pred_neg  # Replace with FN model results\n",
    "\n",
    "#Compute performance for the Combined Model\n",
    "sensitivity_combined = recall_score(y_true, y_pred_combined)\n",
    "precision_combined = precision_score(y_true, y_pred_combined)\n",
    "\n",
    "print(\"Baseline Sensitivity (Recall):\", sensitivity_baseline)\n",
    "print(\"Baseline Precision (PPV):\", precision_baseline)\n",
    "print(\"Combined Model Sensitivity (Recall):\", sensitivity_combined)\n",
    "print(\"Combined Model Precision (PPV):\", precision_combined)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(sensitivity_baseline, precision_baseline, \n",
    "            label='Original 12SL Algorithm', color='blue', marker='o', s=100)\n",
    "plt.scatter(sensitivity_combined, precision_combined, \n",
    "            label='Corrected Model (TP + FN)', color='red', marker='x', s=100)\n",
    "\n",
    "plt.xlabel('Sensitivity (Recall)')\n",
    "plt.ylabel('Positive Predictive Value (Precision)')\n",
    "plt.title('Sensitivity vs. PPV: Original vs Corrected Model')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
