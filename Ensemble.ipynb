{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1toPkv5kUQn8YfqaruGyC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clv07/stroke-of-luck/blob/Data-import/Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VOJxQEz83iVy"
      },
      "outputs": [],
      "source": [
        "# LOAD IN DATA\n",
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('positive.csv', na_values=['NULL'])\n",
        "\n",
        "df2 = pd.read_csv('negative.csv', na_values=['NULL'])\n",
        "\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "df['AcquisitionDateTime_DT'] = pd.to_datetime(df['AcquisitionDateTime_DT'])\n",
        "\n",
        "#print(df.head())\n",
        "#print(df.info())\n",
        "#print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GET MODEL TO WORK IN COLAB W/ GPU (GO TO EDIT -> NOTEBOOK SETTINGS -> GPU)\n",
        "!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd"
      ],
      "metadata": {
        "id": "lBgWrcwH7GJG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "RBggYsos3sG9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "y = df[\"MI_Phys\"]\n",
        "\n",
        "X = df.drop(columns=[\"PatientID\", \"12SL_Codes\", \"Phys_Codes\", \"TestID\", \"Source\",\n",
        "                     \"Gender\", \"PatientAge\", \"AcquisitionDateTime_DT\", \"MI_Phys\", \"POffset\", \"PAxis\", \"POnset\"])\n",
        "X = X.loc[:, ~X.columns.str.contains('P_')]\n",
        "X = X.loc[:, ~X.columns.str.contains('Full')]\n",
        "X = X.loc[:, ~X.columns.str.contains('Rate')]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Save the original algorithm's prediction for test set before dropping it\n",
        "y_12SL = X_test[\"MI_12SL\"]\n",
        "\n",
        "# Split data based on MI_12SL classification\n",
        "X_train_pos = X_train[X_train[\"MI_12SL\"] == 1].drop(columns=[\"MI_12SL\"])\n",
        "X_train_neg = X_train[X_train[\"MI_12SL\"] == 0].drop(columns=[\"MI_12SL\"])\n",
        "X_test_pos = X_test[X_test[\"MI_12SL\"] == 1].drop(columns=[\"MI_12SL\"])\n",
        "X_test_neg = X_test[X_test[\"MI_12SL\"] == 0].drop(columns=[\"MI_12SL\"])\n",
        "\n",
        "# Ensure y labels match the correct samples\n",
        "y_train_pos = y_train.loc[X_train_pos.index]  # True positives or false positives\n",
        "y_train_neg = y_train.loc[X_train_neg.index]  # True negatives or false negatives\n",
        "y_test_pos = y_test.loc[X_test_pos.index]\n",
        "y_test_neg = y_test.loc[X_test_neg.index]\n",
        "\n",
        "X_train = X_train.drop(columns=[\"MI_12SL\"])\n",
        "X_test = X_test.drop(columns=[\"MI_12SL\"])\n",
        "\n",
        "# Extract MI_12SL predictions\n",
        "y_12SL_pos = y_12SL.loc[X_test_pos.index]  # Original classifier's labels\n",
        "y_12SL_neg = y_12SL.loc[X_test_neg.index]"
      ],
      "metadata": {
        "id": "XGCWyyMs3ysH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positive Model\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'dart',\n",
        "    'colsample_bytree': 0.9889,\n",
        "    'n_estimators': 1000,\n",
        "    'learning_rate': 0.0807,\n",
        "    'random_state': 42,\n",
        "    'verbose': -1,\n",
        "    'num_leaves': 106,\n",
        "    'min_child_weight': 1,\n",
        "    'max_depth' : 7,\n",
        "    'reg_alpha' : 0.7909,\n",
        "    'reg_lambda' : 0.61348,\n",
        "    'subsample' : 0.903,\n",
        "    # 'metric': 'auc',\n",
        "    'scale_pos_weight': 3.537,\n",
        "    'device': 'gpu',\n",
        "    }\n",
        "model_pos = (lgb.LGBMClassifier(**lgb_params))\n",
        "model_pos.fit(X_train_pos, y_train_pos)\n",
        "y_pred_pos = model_pos.predict(X_test_pos)\n",
        "score = f1_score(y_test_pos, y_pred_pos, average='micro')\n",
        "print(\"Final F1 score: \", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdrH_8w45HCf",
        "outputId": "93057558-ff19-4f20-fd48-5aee47c82200"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final F1 score:  0.7859021567596002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add predictions as a new column to the test set\n",
        "X_test_pos_with_preds = X_test_pos.copy()\n",
        "X_test_pos_with_preds[\"MI_Predicted\"] = y_pred_pos\n",
        "\n",
        "# Get original data rows from `df` that correspond to the same indices as X_test_pos\n",
        "df_test_pos_original = df.loc[X_test_pos.index]\n",
        "\n",
        "# Join predictions back to the original dataset\n",
        "df_with_predictions = df_test_pos_original.copy()\n",
        "df_with_predictions[\"MI_Predicted\"] = y_pred_pos\n",
        "\n",
        "# Separate into positive and negative predictions\n",
        "df_mi_detected = df_with_predictions[df_with_predictions[\"MI_Predicted\"] == 1]\n",
        "df_no_mi_detected = df_with_predictions[df_with_predictions[\"MI_Predicted\"] == 0]"
      ],
      "metadata": {
        "id": "vDzIKUJU8rI-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Negative Model\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'n_estimators': 200,\n",
        "    'learning_rate': 0.1,\n",
        "    'random_state': 42,\n",
        "    'verbose': -1,\n",
        "    'num_leaves': 127,\n",
        "    'scale_pos_weight': 10,\n",
        "    'device': 'gpu',\n",
        "    }\n",
        "model_neg = (lgb.LGBMClassifier(**lgb_params))\n",
        "model_neg.fit(X_train_neg, y_train_neg)\n",
        "y_pred_neg = model_neg.predict(X_test_neg)\n",
        "score = f1_score(y_test_neg, y_pred_neg, average='micro')\n",
        "print(\"Final F1 score: \", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oovO_QVX5vmm",
        "outputId": "2630701e-2e62-41d8-a892-36f98b276d75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final F1 score:  0.9538450195384502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add predictions to the test set\n",
        "X_test_neg_with_preds = X_test_neg.copy()\n",
        "X_test_neg_with_preds[\"MI_Predicted\"] = y_pred_neg\n",
        "\n",
        "# Get corresponding original rows from df\n",
        "df_test_neg_original = df.loc[X_test_neg.index]\n",
        "\n",
        "# Join predictions back to the original dataset\n",
        "df_with_predictions_neg = df_test_neg_original.copy()\n",
        "df_with_predictions_neg[\"MI_Predicted\"] = y_pred_neg\n",
        "\n",
        "# Separate based on prediction outcome\n",
        "df_mi_detected_neg = df_with_predictions_neg[df_with_predictions_neg[\"MI_Predicted\"] == 1]\n",
        "df_no_mi_detected_neg = df_with_predictions_neg[df_with_predictions_neg[\"MI_Predicted\"] == 0]"
      ],
      "metadata": {
        "id": "dCZHuMGJ8mkR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine the two datasets\n",
        "df_combined_no_mi = pd.concat([df_no_mi_detected, df_no_mi_detected_neg], ignore_index=True)\n",
        "\n",
        "#Drop any prediction columns or non-feature columns\n",
        "df_combined_no_mi = df_combined_no_mi.drop(columns=[\"MI_Predicted\"], errors=\"ignore\")\n",
        "\n",
        "#Prepare features for prediction\n",
        "X_combined = df_combined_no_mi.drop(columns=[\"PatientID\", \"12SL_Codes\", \"Phys_Codes\", \"TestID\", \"Source\",\n",
        "                                             \"Gender\", \"PatientAge\", \"AcquisitionDateTime_DT\", \"MI_Phys\",\n",
        "                                             \"POffset\", \"PAxis\", \"POnset\"], errors='ignore')\n",
        "\n",
        "X_combined = X_combined.loc[:, ~X_combined.columns.str.contains('P_')]\n",
        "X_combined = X_combined.loc[:, ~X_combined.columns.str.contains('Full')]\n",
        "X_combined = X_combined.loc[:, ~X_combined.columns.str.contains('Rate')]\n",
        "X_combined = X_combined.drop(columns=[\"MI_12SL\"], errors='ignore')  # also drop MI_12SL if it's still there\n",
        "\n",
        "# Confirm shape matches the model's training input\n",
        "assert X_combined.shape[1] == model_neg.n_features_in_, f\"Expected {model_neg.n_features_in_} features, got {X_combined.shape[1]}\"\n",
        "\n",
        "# Step 4: Predict again\n",
        "y_combined_pred = model_neg.predict(X_combined)\n",
        "\n",
        "# Step 5: Reattach predictions\n",
        "df_combined_no_mi[\"MI_Predicted_Again\"] = y_combined_pred\n",
        "\n",
        "# Separate based on predictions\n",
        "df_still_no_mi = df_combined_no_mi[df_combined_no_mi[\"MI_Predicted_Again\"] == 0]\n",
        "df_new_mi_detected = df_combined_no_mi[df_combined_no_mi[\"MI_Predicted_Again\"] == 1]\n"
      ],
      "metadata": {
        "id": "RUHYACuz9FRq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicate prediction columns if they exist\n",
        "df_mi_detected = df_mi_detected.drop(columns=[\"MI_Predicted\"], errors=\"ignore\")\n",
        "df_mi_detected_neg = df_mi_detected_neg.drop(columns=[\"MI_Predicted\"], errors=\"ignore\")\n",
        "df_new_mi_detected = df_new_mi_detected.drop(columns=[\"MI_Predicted_Again\"], errors=\"ignore\")\n",
        "\n",
        "# Combine all MI-detected datasets\n",
        "df_all_mi_detected = pd.concat(\n",
        "    [df_mi_detected, df_mi_detected_neg, df_new_mi_detected],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "# drop duplicates by PatientID\n",
        "df_all_mi_detected = df_all_mi_detected.drop_duplicates(subset=[\"PatientID\"])"
      ],
      "metadata": {
        "id": "LxEbsiid97z0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tag predictions based on ground truth\n",
        "df_all_mi_detected[\"Prediction\"] = \"Positive\"\n",
        "df_still_no_mi[\"Prediction\"] = \"Negative\"\n",
        "\n",
        "#Combine both datasets for evaluation\n",
        "df_eval = pd.concat([df_all_mi_detected, df_still_no_mi], ignore_index=True)\n",
        "\n",
        "#Classify each row based on actual vs. predicted\n",
        "def classify_row(row):\n",
        "    if row[\"MI_Phys\"] == 1 and row[\"Prediction\"] == \"Positive\":\n",
        "        return \"True Positive\"\n",
        "    elif row[\"MI_Phys\"] == 0 and row[\"Prediction\"] == \"Positive\":\n",
        "        return \"False Positive\"\n",
        "    elif row[\"MI_Phys\"] == 0 and row[\"Prediction\"] == \"Negative\":\n",
        "        return \"True Negative\"\n",
        "    elif row[\"MI_Phys\"] == 1 and row[\"Prediction\"] == \"Negative\":\n",
        "        return \"False Negative\"\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "df_eval[\"PredictionType\"] = df_eval.apply(classify_row, axis=1)\n",
        "\n",
        "#Count results\n",
        "summary = df_eval[\"PredictionType\"].value_counts()\n",
        "print(\"Performance Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPX1HvtG_e0p",
        "outputId": "7afa220b-cd60-4c15-f7ac-157fae04c637"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-b1b13378ab88>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_still_no_mi[\"Prediction\"] = \"Negative\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Summary:\n",
            "PredictionType\n",
            "True Negative     13399\n",
            "True Positive      1007\n",
            "False Negative      587\n",
            "False Positive      471\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map prediction types to binary labels for evaluation\n",
        "df_eval[\"MI_Predicted_Final\"] = df_eval[\"PredictionType\"].map({\n",
        "    \"True Positive\": 1,\n",
        "    \"False Positive\": 1,\n",
        "    \"True Negative\": 0,\n",
        "    \"False Negative\": 0\n",
        "})\n",
        "\n",
        "y_true = df_eval[\"MI_Phys\"]\n",
        "y_pred = df_eval[\"MI_Predicted_Final\"]\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX9PEO2jAh_i",
        "outputId": "e6555cca-d598-4571-c571-d6b85e0fe961"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[13399   471]\n",
            " [  587  1007]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96     13870\n",
            "           1       0.68      0.63      0.66      1594\n",
            "\n",
            "    accuracy                           0.93     15464\n",
            "   macro avg       0.82      0.80      0.81     15464\n",
            "weighted avg       0.93      0.93      0.93     15464\n",
            "\n",
            "Accuracy: 0.931583031557165\n",
            "F1 Score: 0.6555989583333334\n"
          ]
        }
      ]
    }
  ]
}