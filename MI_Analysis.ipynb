{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clv07/stroke-of-luck/blob/Data-import/MI_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuDAMsAImOhg",
        "outputId": "78da5686-7f9f-4668-f795-10cb71f0c421"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dx_mapping_df = pd.read_csv(\"./dx_mapping_unscored.csv\")\n",
        "dx_to_snomed_dict = dict(zip(dx_mapping_df[\"Dx\"], dx_mapping_df[\"SNOMEDCTCode\"]))\n",
        "\n",
        "statements_df = pd.read_csv(\"./12sl_statements.csv\")\n",
        "statements_df[\"Full Text\"] = statements_df[\"Full Text\"].fillna(\"No Full Text\")\n",
        "statement_number_to_text_dict = dict(zip(statements_df[\"Statement Number\"], statements_df[\"Full Text\"]))\n",
        "\n",
        "\n",
        "print(dx_to_snomed_dict)\n",
        "print(statement_number_to_text_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "-YcrPCe2Qz3g",
        "outputId": "298b458a-0954-4520-c2c4-3e04b0fa9d4a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# STEP 1: READ AND PROCESS EACH 12SL CSV\n",
        "###############################################################################\n",
        "def standardize_patient_id(patient_id):\n",
        "    \"\"\"\n",
        "    Standardizes the PatientID:\n",
        "    - If it ends with '_hr' (e.g., '00001_hr'), it is converted to 'HR00001'.\n",
        "    - If it's already correctly formatted (e.g., 'A00001'), it remains unchanged.\n",
        "    \"\"\"\n",
        "    if isinstance(patient_id, str) and re.match(r\"^\\d+_hr$\", patient_id):\n",
        "        numeric_part = patient_id.split(\"_\")[0]  # Extract '00001'\n",
        "        return f\"HR{numeric_part}\"  # Convert to 'HR00001'\n",
        "    return patient_id  # Leave unchanged if already correct\n",
        "\n",
        "\n",
        "def read_and_process_12sl(folder_path, dataset_name=None):\n",
        "    \"\"\"\n",
        "    Reads and processes the 12SL data from a given folder, which contains:\n",
        "    - meas.csv (has TestID and PatientID)\n",
        "    - 12slv24_stt.csv (has TestID and statement codes)\n",
        "\n",
        "    Parameters:\n",
        "    - folder_path (str): The folder containing the two CSV files.\n",
        "    - dataset_name (str, optional): The dataset name.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Processed DataFrame with 'PatientID', 'Statement_codes', and 'Source'.\n",
        "    \"\"\"\n",
        "    meas_path = os.path.join(folder_path, \"meas.csv\")\n",
        "    stt_path = os.path.join(folder_path, \"12slv24_stt.csv\")\n",
        "\n",
        "    if not os.path.exists(meas_path) or not os.path.exists(stt_path):\n",
        "        print(f\"Warning: One or both files missing in {folder_path}. Skipping.\")\n",
        "        return pd.DataFrame(columns=[\"PatientID\", \"Statement_codes\", \"Source\"])  # Return empty if missing files\n",
        "\n",
        "    # Read meas.csv (contains TestID and PatientID)\n",
        "    df_meas = pd.read_csv(meas_path)\n",
        "\n",
        "    # Read 12slv24_stt.csv (contains TestID and codes)\n",
        "    df_stt = pd.read_csv(stt_path)\n",
        "\n",
        "    # Ensure 'TestID' and 'PatientID' are correctly named\n",
        "    first_col_meas = df_meas.columns[0]  # TestID\n",
        "    second_col_meas = df_meas.columns[1]  # PatientID\n",
        "    df_meas.rename(columns={first_col_meas: \"TestID\", second_col_meas: \"PatientID\"}, inplace=True)\n",
        "\n",
        "    first_col_stt = df_stt.columns[0]  # TestID\n",
        "    df_stt.rename(columns={first_col_stt: \"TestID\"}, inplace=True)\n",
        "\n",
        "    # Merge on TestID, but retain only PatientID\n",
        "    df_raw = pd.merge(df_meas[[\"TestID\", \"PatientID\"]], df_stt, on=\"TestID\", how=\"inner\")\n",
        "\n",
        "    # Standardize PatientID formatting\n",
        "    df_raw[\"PatientID\"] = df_raw[\"PatientID\"].apply(standardize_patient_id)\n",
        "\n",
        "    # Process Statement Codes\n",
        "    if len(df_raw.columns) == 3:  # If only TestID, PatientID, and one column of statements\n",
        "        df_raw[\"Statement_codes\"] = (\n",
        "            df_raw.iloc[:, 2]\n",
        "            .astype(str)\n",
        "            .str.strip(\"|\")\n",
        "            .str.replace(\"|\", \",\")\n",
        "        )\n",
        "    else:  # If multiple statement code columns exist\n",
        "        code_cols = df_raw.columns[2:]\n",
        "        df_raw[\"Statement_codes\"] = df_raw[code_cols].apply(\n",
        "            lambda row: \",\".join(row.dropna().astype(str)), axis=1\n",
        "        )\n",
        "\n",
        "    # Keep only PatientID and Statement_codes\n",
        "    df_processed = df_raw[[\"PatientID\", \"Statement_codes\"]].copy()\n",
        "\n",
        "\n",
        "    # Add Source column\n",
        "    if dataset_name:\n",
        "        df_processed[\"Source\"] = dataset_name\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "# Define dataset folder paths\n",
        "ptbxl_folder = \"./PTBXL\"\n",
        "shaoxing_folder = \"./Shaoxing\"\n",
        "cpsc_folder = \"./CPSC2018\"\n",
        "\n",
        "# Process each dataset\n",
        "df_ptbxl = read_and_process_12sl(ptbxl_folder, dataset_name=\"PTBXL\")\n",
        "df_shaoxing = read_and_process_12sl(shaoxing_folder, dataset_name=\"Shaoxing\")\n",
        "df_cpsc = read_and_process_12sl(cpsc_folder, dataset_name=\"CPSC2018\")\n",
        "\n",
        "# Concatenate all 12SL data.\n",
        "df_12sl_combined = pd.concat([df_ptbxl, df_shaoxing, df_cpsc], ignore_index=True)\n",
        "\n",
        "print(df_12sl_combined.head())\n",
        "\n",
        "###############################################################################\n",
        "# STEP 2: READ AND PREPARE THE results.csv (PHYSICIAN CODES)\n",
        "###############################################################################\n",
        "results_csv = r\"./results.csv\"\n",
        "df_results = pd.read_csv(results_csv)\n",
        "\n",
        "# The results.csv sample has headers: patient_num and codes.\n",
        "# Rename these to \"TestID\" and \"Statements_Phys\" so that we can merge.\n",
        "df_results.rename(columns={\"patient_num\": \"PatientID\", \"codes\": \"Statements_Phys\"}, inplace=True)\n",
        "\n",
        "# It may be necessary to strip any extra spaces from the codes:\n",
        "df_results[\"Statements_Phys\"] = df_results[\"Statements_Phys\"].astype(str).str.strip()\n",
        "\n",
        "###############################################################################\n",
        "# STEP 3: MERGE THE 12SL DATA WITH THE PHYSICIAN DATA\n",
        "###############################################################################\n",
        "# Merge on the patient ID. Use inner join if you want only matching IDs.\n",
        "df_merged = pd.merge(df_12sl_combined, df_results, on=\"PatientID\", how=\"inner\")\n",
        "\n",
        "###############################################################################\n",
        "# STEP 4: DEFINE MI CODE SETS AND CREATE BINARY INDICATORS\n",
        "###############################################################################\n",
        "# Example MI code sets (update these with the actual codes you consider for MI)\n",
        "MI_codes_12SL = {\n",
        "    700, 740, 760, 780, 801, 810, 820, 826, 827, 828, 829,\n",
        "    920, 930, 940, 950, 960, 961, 962, 963, 964, 965, 966, 967, 968,\n",
        "    # ... add others as needed\n",
        "}\n",
        "\n",
        "MI_codes_Phys = {\n",
        "    57054005, 413444003, 426434006, 54329005, 425419005,\n",
        "    425623009, 164865005, 164861001,\n",
        "    # ... add others as needed\n",
        "}\n",
        "# Pairs matching codes from different systems to common condition, formatted\n",
        "# \"Conditon\" : [num_one, num_two ...] , use to identify shared diagnoses\n",
        "# between physicians and 12SL\n",
        "MI_code_mapping = {\n",
        "    \"anterior infarct\": [740, 810, 820, 54329005],\n",
        "    \"septal infarct\": [700, 810],\n",
        "    \"lateral infarct\": [760, 820, 425623009],\n",
        "    \"inferior infarct\": [780, 801, 806, 425419005],\n",
        "    \"posterior infarct\": [801, 802, 803],\n",
        "    \"infarct - ST elevation\": [826, 827, 963, 964, 965, 966, 967, 968],\n",
        "    \"acute MI or injury\": [4, 821, 822, 823, 826, 827, 828, 829, 920, 930, 940, 950, 960, 961, 962, 1361, 57054005, 413444003, 426434006, 164865005, 164861001]\n",
        "}\n",
        "\n",
        "def flag_mi(codes_str, mi_set):\n",
        "    \"\"\"\n",
        "    Given a string of codes separated by commas (possibly with spaces),\n",
        "    returns 1 if any code (converted to int) is in the mi_set, else 0.\n",
        "    \"\"\"\n",
        "    if pd.isna(codes_str) or codes_str.strip() == \"\":\n",
        "        return 0\n",
        "    codes = [c.strip() for c in codes_str.split(\",\") if c.strip()]\n",
        "    for code in codes:\n",
        "        try:\n",
        "            if int(code) in mi_set:\n",
        "                return 1\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return 0\n",
        "\n",
        "def GivenXInspection(df, codes_str, mi_column, mi_code_mapping):\n",
        "    \"\"\"\n",
        "    Given a Merged DF with 1) Patient Labels, 2) 12SL Codes, 3) Physician Codes,\n",
        "    4) MI_12SL, 5) MI_Phys, determine the probability of 12SL correctly identifying\n",
        "    a symptom given a set of codes (i.e., Probability of 12SL detecting MI given ST Elevation).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The merged dataframe containing MI flags.\n",
        "        codes_str (str): A condition category representing the \"given\" signal (e.g., \"anterior infarct\").\n",
        "        mi_column (str): The column name representing the 12SL calculated Condition flag (e.g., \"MI_12SL\").\n",
        "        mi_code_mapping (dict): A dictionary mapping condition categories to a list of associated codes.\n",
        "\n",
        "    Returns:\n",
        "        float: Probability of correct identification.\n",
        "    \"\"\"\n",
        "    # Get the relevant codes for the given condition\n",
        "    condition_codes = set(mi_code_mapping.get(codes_str, []))\n",
        "\n",
        "    # Convert Statements_Phys to sets for efficient lookup\n",
        "    df[\"Statements_Phys_Set\"] = df[\"Statements_Phys\"].apply(lambda x: set(map(int, x.split(','))) if pd.notna(x) else set())\n",
        "\n",
        "    # Identify cases where at least one given condition code is present in the physician's statements\n",
        "    signal_present = df[df[\"Statements_Phys_Set\"].apply(lambda codes: not condition_codes.isdisjoint(codes))]\n",
        "\n",
        "    # Further filter cases where the physician also flagged MI\n",
        "    mi_positive_patients = signal_present.loc[signal_present[\"MI_Phys\"] == 1, \"PatientID\"]\n",
        "\n",
        "    if mi_positive_patients.empty:\n",
        "        return 0  # Avoid division by zero if no cases exist\n",
        "\n",
        "    # Filter 12SL-flagged cases only for identified patients\n",
        "    correct_identifications = df.loc[df[\"PatientID\"].isin(mi_positive_patients), mi_column].sum()\n",
        "\n",
        "    # Calculate probability\n",
        "    probability = correct_identifications / len(mi_positive_patients)\n",
        "\n",
        "    return probability\n",
        "\n",
        "def LabelMapping(df_merged, mi_code_mapping):\n",
        "    \"\"\"\n",
        "    Given a Merged DF with 1) Patient Labels, 2) 12SL Codes, 3) Physician Codes,\n",
        "    4) MI_12SL, 5) MI_Phys, break down the identified signs by 12SL & Physicians\n",
        "    to categorize label percentages in missed and false flag cases.\n",
        "\n",
        "    Args:\n",
        "        df_merged (pd.DataFrame): The merged dataframe containing MI-related flags.\n",
        "        mi_code_mapping (dict): A dictionary mapping condition categories to lists of associated codes.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Two DataFrames -\n",
        "            - df_code_percentages: Breakdown of missed/false flag percentages per code.\n",
        "            - df_signal_percentages: Breakdown of missed/false flag percentages per condition category.\n",
        "    \"\"\"\n",
        "\n",
        "    # Gather cases flagged by physicians but missed by 12SL\n",
        "    df_missed_by_12sl = df_merged[(df_merged[\"MI_Phys\"] == 1) & (df_merged[\"MI_12SL\"] == 0)]\n",
        "\n",
        "    # Gather cases flagged by 12SL but not confirmed by physicians\n",
        "    df_false_id = df_merged[(df_merged[\"MI_12SL\"] == 1) & (df_merged[\"MI_Phys\"] == 0)]\n",
        "\n",
        "    # Function to count occurrences of each code\n",
        "    def count_codes(df):\n",
        "        code_counts = {}\n",
        "        for codes in df[\"Statements_Phys\"].dropna():\n",
        "            for code in map(int, codes.split(',')):  # Convert to int to ensure consistency\n",
        "                code_counts[code] = code_counts.get(code, 0) + 1\n",
        "        return code_counts\n",
        "\n",
        "    # Count occurrences in missed and false flag cases\n",
        "    missed_code_counts = count_codes(df_missed_by_12sl)\n",
        "    false_code_counts = count_codes(df_false_id)\n",
        "\n",
        "    # Convert counts to DataFrames\n",
        "    df_code_percentages = pd.DataFrame([\n",
        "        {\"Code\": code,\n",
        "         \"Missed_Percentage\": missed_code_counts.get(code, 0) / len(df_missed_by_12sl) * 100 if len(df_missed_by_12sl) > 0 else 0,\n",
        "         \"False_Flag_Percentage\": false_code_counts.get(code, 0) / len(df_false_id) * 100 if len(df_false_id) > 0 else 0}\n",
        "        for code in set(missed_code_counts) | set(false_code_counts)  # Include all unique codes\n",
        "    ])\n",
        "\n",
        "    # Aggregate by signal type (condition category)\n",
        "    signal_counts = {}\n",
        "    for condition, codes in mi_code_mapping.items():\n",
        "        signal_counts[condition] = {\n",
        "            \"Missed_Percentage\": sum(missed_code_counts.get(code, 0) for code in codes) / len(df_missed_by_12sl) * 100 if len(df_missed_by_12sl) > 0 else 0,\n",
        "            \"False_Flag_Percentage\": sum(false_code_counts.get(code, 0) for code in codes) / len(df_false_id) * 100 if len(df_false_id) > 0 else 0\n",
        "        }\n",
        "\n",
        "    df_signal_percentages = pd.DataFrame.from_dict(signal_counts, orient=\"index\").reset_index().rename(columns={\"index\": \"Condition\"})\n",
        "\n",
        "    return df_code_percentages, df_signal_percentages\n",
        "# Create MI flags for 12SL and Physician.\n",
        "df_merged[\"MI_12SL\"] = df_merged[\"Statement_codes\"].apply(lambda x: flag_mi(x, MI_codes_12SL))\n",
        "df_merged[\"MI_Phys\"]  = df_merged[\"Statements_Phys\"].apply(lambda x: flag_mi(x, MI_codes_Phys))\n",
        "\n",
        "###############################################################################\n",
        "# STEP 5: INSPECTION / ANALYSIS\n",
        "###############################################################################\n",
        "print(\"Merged Data Sample:\")\n",
        "print(df_merged.head(10))\n",
        "print(df_merged.columns)\n",
        "print(f\"Total merged records: {len(df_merged)}\")\n",
        "print(f\"Total MI flagged by 12SL: {df_merged['MI_12SL'].sum()}\")\n",
        "print(f\"Total MI flagged by Physician: {df_merged['MI_Phys'].sum()}\")\n",
        "\n",
        "# For example, cases flagged by Physician but not by 12SL (False Negative):\n",
        "df_missed_by_12sl = df_merged[(df_merged[\"MI_Phys\"] == 1) & (df_merged[\"MI_12SL\"] == 0)]\n",
        "print(f\"Records where Physician flagged MI but 12SL did not: {len(df_missed_by_12sl)}\")\n",
        "fn = len(df_missed_by_12sl)\n",
        "\n",
        "# Case 2) Cases Flagged by 12SL but not by Physician (False Positive)\n",
        "df_false_id = df_merged[(df_merged[\"MI_12SL\"] == 1) & (df_merged[\"MI_Phys\"] == 0)]\n",
        "fp = len(df_false_id)\n",
        "print(f\"Records where 12SL flagged MI but Physician did not: {len(df_false_id)}\")\n",
        "\n",
        "# Number of true negatives\n",
        "df_negative = df_merged[(df_merged[\"MI_12SL\"] == 0)]\n",
        "print(f\"True negative MI diagnosis: {len(df_negative)}\")\n",
        "tn = len(df_negative)\n",
        "# Number of true positives\n",
        "df_positive = df_merged[(df_merged[\"MI_12SL\"] == 1)]\n",
        "tp = len(df_positive)\n",
        "print(f\"True positive MI diagnosis: {len(df_positive)}\")\n",
        "\n",
        "# False Positive Percent\n",
        "fp_percent = fp / (tn + fp) * 100\n",
        "fn_percent = fn / (tp + fn) * 100\n",
        "print(f\"false positive rate: {fp_percent}\")\n",
        "print(f\"False negative rate: {fn_percent}\")\n",
        "\n",
        "# Case 3)\n",
        "# a) Given STEMI, What is the chance that 12SL calculated it correctly\n",
        "prob_given_STElevation = GivenXInspection(df_merged,\"infarct - ST elevation\",\"MI_12SL\", MI_code_mapping)\n",
        "print(f\"Probability of 12SL identifying MI given ST Elevation: {prob_given_STElevation * 100} %\")\n",
        "\n",
        "# b) Given MI, What is the chance that 12SL calculated it correctly\n",
        "prob_given_lateral = GivenXInspection(df_merged,\"acute MI or injury\",\"MI_12SL\", MI_code_mapping)\n",
        "print(f\"Probability of 12SL identifying MI given MI or Injury: {prob_given_lateral * 100}\")\n",
        "\n",
        "# Conditional Breakdown of Missed regions\n",
        "breakdown = LabelMapping(df_merged, MI_code_mapping)\n",
        "df_by_code = breakdown[0]\n",
        "df_by_signal = breakdown[1]\n",
        "#print(df_by_code.sort_values(by='False_Flag_Percentage', ascending=False))\n",
        "#print(df_by_code.sort_values(by='Missed_Percentage', ascending=False))\n",
        "#print(df_by_signal.sort_values(by='Missed_Percentage', ascending=False))\n",
        "\n",
        "###############################################################################\n",
        "# STEP 6: GRAPHS\n",
        "###############################################################################\n",
        "\n",
        "# GRAPH 1) MISSED PERCENTAGE (DESCENDING) VS CODE\n",
        "ax1 = df_by_code.sort_values(by=\"Missed_Percentage\",ascending=False).head(10).plot.bar(x='Code', y='Missed_Percentage')\n",
        "ax1.set_xlabel(\"Codes\")\n",
        "ax1.set_ylabel(\"False Negative Rate (%)\")\n",
        "plt.title('12SL False Negative Rate (%) versus code')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "# GRAPH 2) FALSE FLAG (DESCENDING) VS CODE\n",
        "ax1 = df_by_code.sort_values(by=\"False_Flag_Percentage\",ascending=False).head(10).plot.bar(x='Code', y='False_Flag_Percentage')\n",
        "ax1.set_xlabel(\"Codes\")\n",
        "ax1.set_ylabel(\"False Positive Rate (%)\")\n",
        "plt.title('12SL False Positive rate (%) versus Code')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# STEP 7: OUTPUT\n",
        "###############################################################################\n",
        "# Update DF to generate two CSV's, Positive 12SL MI Flags and Negative Flags, and 12SL measurements\n",
        "\n",
        "# Trim and Rename\n",
        "df_merged = df_merged.drop('Statements_Phys_Set', axis=1)\n",
        "df_merged.rename(columns={'Statement_codes' : '12SL_Codes', 'Statements_Phys' : 'Phys_Codes'}, inplace= True)\n",
        "\n",
        "# Refine Measurement Files\n",
        "\n",
        "meas_path = \"./PTBXL/meas.csv\"\n",
        "df_meas_PT = pd.read_csv(meas_path)\n",
        "df_meas_PT[\"PatientID\"] = df_meas_PT[\"PatientID\"].apply(standardize_patient_id)\n",
        "\n",
        "meas_path = \"./Shaoxing/meas.csv\"\n",
        "df_meas_SH = pd.read_csv(meas_path)\n",
        "df_meas_SH[\"PatientID\"] = df_meas_SH[\"PatientID\"].apply(standardize_patient_id)\n",
        "\n",
        "meas_path = \"./CPSC2018/meas.csv\"\n",
        "df_meas_CS = pd.read_csv(meas_path)\n",
        "df_meas_CS[\"PatientID\"] = df_meas_CS[\"PatientID\"].apply(standardize_patient_id)\n",
        "\n",
        "df_meas = pd.concat([df_meas_CS,df_meas_PT,df_meas_SH],axis=0)\n",
        "\n",
        "# Join on PatientIDs\n",
        "df_merged = pd.merge(df_merged,df_meas,how=\"left\", on=[\"PatientID\"])\n",
        "\n",
        "# Seperate Into Two CSV's\n",
        "\n",
        "# a) Positive MI\n",
        "df_positive = df_merged[df_merged[\"MI_12SL\"] == 1]\n",
        "# b) Negative MI\n",
        "df_negative = df_merged[df_merged[\"MI_12SL\"] == 0]\n",
        "\n",
        "df_merged.to_csv(\"dataset.csv\", index=False)\n",
        "df_positive.to_csv(\"positive.csv\", index=False)\n",
        "df_negative.to_csv(\"negative.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QrfqdXNKP2n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dx_mapping_df = pd.read_csv(\"./dx_mapping_unscored.csv\")\n",
        "dx_to_snomed_dict = dict(zip(dx_mapping_df[\"Dx\"], dx_mapping_df[\"SNOMEDCTCode\"]))\n",
        "\n",
        "statements_df = pd.read_csv(\"./12sl_statements.csv\")\n",
        "statements_df[\"Full Text\"] = statements_df[\"Full Text\"].fillna(\"No Full Text\")\n",
        "statement_number_to_text_dict = dict(zip(statements_df[\"Statement Number\"], statements_df[\"Full Text\"]))\n",
        "\n",
        "def translate_codes_to_text(df, code_column, mapping_dict):\n",
        "    df[code_column] = df[code_column].apply(lambda x: mapping_dict.get(x, x) if pd.notna(x) else x)\n",
        "    return df\n",
        "\n",
        "def plot_graph(df, x_column, y_column, title, xlabel, ylabel):\n",
        "    df_translated = translate_codes_to_text(df, x_column, dx_to_snomed_dict)\n",
        "    ax = df_translated.sort_values(by=y_column, ascending=False).head(10).plot.bar(x=x_column, y=y_column)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhlDqXtE5aDz"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# STEP 8: TEST\n",
        "###############################################################################\n",
        "# Test cases to verify datasets are being properly joined\n",
        "\n",
        "def read_file(folder, file):\n",
        "    return pd.read_csv(os.path.join(folder, file))\n",
        "\n",
        "meas_name = \"meas.csv\"\n",
        "stt_name = \"12slv24_stt.csv\"\n",
        "sources = [\"Shaoxing\", \"PTBXL\", \"CPSC2018\"]\n",
        "\n",
        "datasets = {source: (read_file(source, meas_name), read_file(source, stt_name)) for source in sources}\n",
        "\n",
        "output_df = pd.read_csv(\"merged_output.csv\")\n",
        "physician_diag = pd.read_csv(\"results.csv\")\n",
        "\n",
        "original_id = pd.concat([df[\"PatientID\"] for df, _ in datasets.values()])\n",
        "output_id = output_df[\"PatientID\"]\n",
        "\n",
        "# No missing entries\n",
        "assert sorted(original_id) == sorted(output_id), \"Missing entries\"\n",
        "\n",
        "# Duplicate Patient ID\n",
        "# assert len(output_id) == len(output_id.unique()), \"Duplicate Patient ID\" # failing\n",
        "visit, dup = set(), []\n",
        "for id in output_id:\n",
        "    dup.append(id) if id in visit else visit.add(id)\n",
        "\n",
        "print(f\"Duplicate Patient ID: {dup}\")\n",
        "for id in dup:\n",
        "    print(output_df[output_df[\"PatientID\"] == id])\n",
        "\n",
        "# Correct format of Patient ID\n",
        "id_format = r\"^[A-Z]+\\d+$\"\n",
        "assert output_df[\"PatientID\"].str.match(id_format).all(), \"Incorrect format Patient ID\"\n",
        "\n",
        "def convert_format(s):\n",
        "    return re.sub(r\"^(\\d+)_([a-z]+)$\", lambda m: m.group(2).upper() + m.group(1), s)\n",
        "\n",
        "for source, (meas, stt) in datasets.items():\n",
        "    output = output_df[output_df[\"Source\"] == source]\n",
        "    original_id = meas[\"PatientID\"].apply(convert_format) if source == \"PTBXL\" else meas[\"PatientID\"]\n",
        "    assert sorted(output_id.tolist()) == sorted(original_id.tolist()), f\"Mismatch found for {source}\"\n",
        "\n",
        "    # Physician code mapping\n",
        "    for id in output_id:\n",
        "        original_phys = physician_diag[physician_diag[\"patient_num\"] == id][\"codes\"]\n",
        "        output_phys = output_df[output_df[\"PatientID\"] == id][\"Statements_Phys\"]\n",
        "        assert not output_phys.empty, f\"Physicians statement code missing for Patient ID {id}\"\n",
        "        assert sorted(original_phys) == sorted(output_phys), f\"Incorrect physician statement code mapping for Patient ID {id}\"\n",
        "\n",
        "    # 12SL code mapping\n",
        "    merged_df = meas.merge(stt, on=\"TestID\", how=\"inner\")[['PatientID', 'Statements']]\n",
        "    for id in output_id:\n",
        "        original_12sl = merged_df[merged_df[\"PatientID\"] == id][\"Statements\"]\n",
        "        output_12sl = output_df[output_df[\"PatientID\"] == id][\"Statements_12SL\"]\n",
        "        assert not output_12sl.empty, f\"12SL statement code missing for Patient ID {id}\"\n",
        "        assert sorted(original_12sl) == sorted(output_12sl), f\"Incorrect 12SL statement code mapping for Patient ID {id}\"\n",
        "\n",
        "    # Physician diagnosis\n",
        "    for id in output_id:\n",
        "        output_phys = output_df[output_df[\"PatientID\"] == id][\"Statements_Phys\"]\n",
        "        output_phys = [code for code in output_phys if code in MI_codes_Phys]\n",
        "        output_phys_mi = 1 if output_phys else 0\n",
        "        assert \"MI_Phys\" in output_df.columns, f\"Missing MI_Phys column for Patient ID {id}\"\n",
        "        assert output_df.loc[output_df[\"PatientID\"] == id, \"MI_Phys\"].values[0] == output_phys_mi, f\"Incorrect Physician MI flag for Patient ID {id}\"\n",
        "\n",
        "    # 12SL diagnosis\n",
        "    for id in output_id:\n",
        "        output_12sl = output_df[output_df[\"PatientID\"] == id][\"Statement_codes\"]\n",
        "        output_12sl = [code for code in output_12sl if code in MI_codes_12SL]\n",
        "        output_12sl_mi = 1 if output_12sl else 0\n",
        "        assert \"MI_12SL\" in output_df.columns, f\"Missing MI_12SL column for Patient ID {id}\"\n",
        "        assert output_df.loc[output_df[\"PatientID\"] == id, \"MI_12SL\"].values[0] == output_12sl_mi, f\"Incorrect 12SL MI flag for Patient ID {id}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMTIoYpluuAW",
        "outputId": "b61cb31b-48af-465c-c37a-1cce7e141ba8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
